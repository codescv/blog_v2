[
  {
    "objectID": "posts/sd/stable-diffusion-1.html#setup",
    "href": "posts/sd/stable-diffusion-1.html#setup",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Setup",
    "text": "Setup\nTo begin our journey, first we need to install some packages.\n\ndiffusers This is the package that we are mainly using for generating images.\ntransformers This is the package to encode text into embeddings.\nAnd a few other supporting packages to work with the above packages.\n\n\n!pip install -Uqq diffusers transformers ftfy scipy accelerate gradio xformers triton==2.0.0.dev20221120\nimport pathlib\nimport huggingface_hub\nif not pathlib.Path('/root/.huggingface/token').exists():\n  huggingface_hub.notebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#utility-functions",
    "href": "posts/sd/stable-diffusion-1.html#utility-functions",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Utility Functions",
    "text": "Utility Functions\n\nimport PIL\nimport math\n\ndef image_grid(imgs, rows=None, cols=None) -> PIL.Image.Image:\n  n_images = len(imgs)\n  if not rows and not cols:\n    cols = math.ceil(math.sqrt(n_images))\n  if not rows:\n    rows = math.ceil(n_images / cols)\n  if not cols:\n    cols = math.ceil(n_images / rows)\n\n  w, h = imgs[0].size\n  grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n\n  for i, img in enumerate(imgs):\n      grid.paste(img, box=(i%cols*w, i//cols*h))\n  return grid"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Initializing the Pipeline",
    "text": "Initializing the Pipeline\nThere are two important params to intialize a Pipeline. a model_id and a revision.\n\nThe model_id can be either a huggingface model id or some path in your file system. To find which ones to use, go to huggingface and look for model id at the top. Later when we train our own models we will pass in the path to our trained model.\nThe revision can be set fp16 to save GPU memory by using 16 bit numbers.\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Generating image using the text to image pipeline",
    "text": "Generating image using the text to image pipeline\nTo generate an image from text, we just call the pipe() method. There are a few arguments to the method:\n\nprompt A text describing the thing you want the image to be.\nnegative_prompt A text describing the features that you don’t want the image to have.\ngenerator This is a random number generator. By default the generator is None, and the output is random for same prompts. A fixed seed allows repeatable experiment (same input-> same output).\nwidth and height the dimension of output image.\nguidance_scale A scale determining the extent of how your image accurately matches your prompt. In practice I find it not very useful to tune this, just use a 7.5 you will be fine.\nnum_inference_steps How many steps to run the diffusion algorithm(will explain later). The more steps, the better the image quality, and the more time it takes to generate an image.\n\n\ndef text2image(pipe,\n               prompt: str,\n               seed: int,\n               return_grid=False,\n               grid_size=None,\n               **kwargs):\n  generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n  with torch.autocast(\"cuda\"):\n    images = pipe(prompt,\n                  generator=generator,\n                  **kwargs).images\n\n  if len(images) == 1:\n    return images[0]\n  elif return_grid:\n    return image_grid(images)\n  else:\n    return images"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#baseline",
    "href": "posts/sd/stable-diffusion-1.html#baseline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Baseline",
    "text": "Baseline\nFirst let’s try using a prompt with all default parameters:\n\nprompt = \"a photo of a woman wearing a red dress\"\nnegative_prompt = \"\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s terrible! If you get completely unusable images like the above, the first thing to change is the prompts."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "href": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Add Prompts and Negative Prompts",
    "text": "Add Prompts and Negative Prompts\nLet’s add a few style keywords to the prompt, and then some negative keywords:\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s far from perfect, but much better than our first try. Generally speaking stable diffusion version 2 works much worse than version 1.5 by default, and it needs much more effort in tuning the prompts.\nThe negative prompt usually works well for many different images, but for the positive prompts there can be a lot of possiblities.\nNow let’s build a systematic way of finding prompt additions:"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "href": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "A random prompt generator",
    "text": "A random prompt generator\n\nimport random\n\nartists = ['Aaron Douglas','Agnes Lawrence Pelton','Akihiko Yoshida','Albert Bierstadt','Albert Bierstadt','Alberto Giacometti',\n           'Alberto Vargas','Albrecht Dürer','Aleksi Briclot','Alex Grey','Alex Horley-Orlandelli','Alex Katz','Alex Ross',\n           'Alex Toth','Alexander Jansson','Alfons Maria Mucha','Alfred Kubin','Alphonse Mucha','Anato Finnstark','Anders Zorn',\n           'André Masson','Andreas Rocha','Andrew Wyeth','Anish Kapoor','Anna Dittmann','Anna Mary Robertson Moses','Anni Albers',\n           'Ansel Adams','Anthony van Dyck','Anton Otto Fischer','Antonio Mancini','April Gornik','Arnold Böcklin','Art Spiegelman',\n           'Artemisia Gentileschi','Artgerm','Arthur Garfield Dove','Arthur Rackham','Asher Brown Durand','Aubrey Beardsley',\n           'Austin Briggs','Ayami Kojima','Bastien Lecouffe-Deharme','Bayard Wu','Beatrix Potter','Beeple','Beksinski',\n           'Bill Sienkiewicz','Bill Ward','Bill Watterson','Bob Eggleton','Boris Vallejo','Brian Bolland','Brian Froud',\n           'Bruce Pennington','Bunny Yeager','Camille Corot','Camille Pissarro','Canaletto','Caravaggio','Caspar David Friedrich',\n           'Cedric Peyravernay','Charles Addams','Charles Dana Gibson','Chesley Bonestell','Chris Foss','Chris Moore',\n           'Chris Rallis','Chriss Foss','Cindy Sherman','Clara Peeters','Claude Monet','Clyde Caldwell','Coles Phillips',\n           'Cornelis Bisschop','Coby Whitmore','Craig Mullins','Cynthia Sheppard','Dale Chihuly','Damien Hirst','Dan Mumford',\n           'Daniel Merriam','Darek Zabrocki','Dave Dorman','Dave Gibbons','Dave McKean','David Firth','Dean Cornwell','Dean Ellis',\n           'Diane Dillon','Disney','Don Maitz','Donato Giancola','Dorothea Tanning','Dreamworks','Dr. Seuss','Earl Norem',\n           'Earle Bergey','Earnst Haeckel','Ed Emshwiller','Edgar Degas','Edmund Dulac','Edmund Leighton','Édouard Manet',\n           'Edvard Munch','Edward Burne-Jones','Edward Gorey','Edward Hopper','Edward Lear','Edwin Austin Abbey','Edwin Deakin',\n           'Egon Schiele','El Greco','Elizabeth Shippen Green','Emmanuel Shiu','Emory Douglas','Esao Andrews','Eugène Delacroix',\n           'Evelyn De Morgan','E.H. Shepard','F. Scott Hess','Fairfield Porter','Federico Pelat','Filippino Lippi','Fitz Henry Lane',\n           'Francis Bacon','Francisco Goya','Frank Frazetta','Frank Xavier Leyendecker','Franklin Booth','Franz Sedlacek',\n           'Frederick Edwin Church','Frederick McCubbin','Gaston Bussière','Gediminas Pranckevicius','Geof Darrow',\n           'George B. Bridgman','George Cruikshank','George Inness','George Luks',\"Georgia O'Keeffe\",'Gerald Brom','Giacomo Balla',\n           'Gil Elvgren','Gillis Rombouts','Giorgio de Chirico','Giorgione','Giovanni Battista Piranesi','Greg Hildebrandt',\n           'Greg Rutkowski','Greg Staples','Gregory Manchess','Guido Borelli da Caluso','Gustaf Tenggren','Gustav Klimt',\n           'Gustave Doré','Gustave Moreau','Gwen John','Hannah Höch','Hans Baldung','Hans Bellmer','Harrison Fisher','Harvey Dunn',\n           'Harvey Kurtzman','Henri de Toulouse-Lautrec','Henri Matisse','Henri Rousseau','Henry Ossawa Tanner','Henry Raleigh',\n           'Hethe Srodawa','Hieronymus Bosch','Hiromu Arakawa','Hokusai','Howard Chandler Christy','Howard Pyle','Hubert Robert',\n           'Hugh Ferriss','Hyun Lee','H.R. Giger','Igor Kieryluk','Igor Morski','Igor Wolkski','Ilya Kuvshinov','Ilya Repin',\n           'Inyi Han','Isaac Levitan','Ivan Aivazovsky','Ivan Albright','Ivan Bilibin','Ivan Shishkin','Jacek Yerka','Jack Kirby',\n           'Jackson Pollock','Jakub Rozalski','James C. Christensen','James Gillray','James Gurney','James Jean','James Paick',\n           'Jamie Hewlett','Jan van Eyck','Janet Fish','Jasper Johns','J.C. Leyendecker','Jean Delville','Jean Giraud',\n           'Jean Metzinger','Jean-Honoré Fragonard','Jean-Michel Basquiat','Jeff Easley','Jeff Koons','Jeffrey Smith',\n           'Jerome Lacoste','Jerry Pinkney','Jesper Ejsing','Jessie Willcox Smith','Jim Burns','Jim Steranko','Joaquín Sorolla',\n           'Joe Jusko','Johannes Vermeer','Johfra Bosschart','John Atkinson Grimshaw','John Bauer','John Berkey','John Constable',\n           'John Frederick Kensett','John French Sloan','John Harris','John Howe','John James Audubon','John Martin',\n           'John Philip Falter','John Romita Jr','Jon Foster','Jon Whitcomb','Joseph Cornell','Juan Gris','Junji Ito',\n           'J.M.W. Turner','Kadir Nelson','Kandinsky','Karol Bak','Kate Greenaway','Kawanabe Kyōsai','Kay Nielsen',\n           'Keith Haring','Kelly Freas','Kelly Mckernan','Kim Jung Gi','Kinuko Craft','Konstantin Vasilyev',\n           'Konstantinas Ciurlionis','Krenz Cushart','Lale Westvind','Larry Elmore','Laura Muntz Lyall','Laurel Burch',\n           'Laurie Lipton','Lawren Harris','Lee Madgwick','Leo and Diane Dillon','Leonora Carrington','Liam Wong','Lise Deharme',\n           'Lois van Baarle','Louis Glackens','Louis Janmot','Louise Bourgeois','Lucian Freud','Luis Royo','Lynda Benglis',\n           'Lyubov Popova','Maciej Kuciara','Makoto Shinkai','Malevich','Marc Simonetti','Margaret Macdonald Mackintosh',\n           'Maria Sibylla Merian','Marianne North','Mario Sanchez Nevado','Mark Ryden','Martin Johnson Heade','Mary Cassatt',\n           'Mati Klarwein','Maxfield Parrish','Mead Schaeffer','Michael Hussar','Michael Parkes','Michael Whelan',\n           'Mikalojus Konstantinas Čiurlionis','Mike Mignola','Milton Caniff','Milton Glaser','Moebius','Mondrian','M.C. Escher',\n           'Noah Bradley','Noriyoshi Ohrai','Norman Rockwell','N.C. Wyeth','Odd Nerdrum','Odilon Redon','Ohara Koson',\n           'Paul Cézanne','Paul Delvaux','Paul Gauguin','Paul Klee','Paul Lehr','Peter Elson','Peter Gric','Peter Helck',\n           'Peter Max','Peter Mohrbacher','Peter Paul Rubens','Pierre Bonnard','Pierre-Auguste Renoir','Pieter Bruegel the Elder',\n           'Pieter Claesz','Pixar','P.A. Works','Rafal Olbinski','Ralph Horsley','Ralph McQuarrie','Randolph Caldecott',\n           'Raphael Lacoste','Ray Caesar','Raymond Swanland','Rebecca Guay','Rembrandt','Rembrandt van Rijn','Rene Magritte',\n           'RHADS','Richard Dadd','Richter','Rob Gonsalves','Robert Delaunay','Robert McCall','Robert McGinnis',\n           'Robert Rauschenberg','Roberto da Matta','Rockwell Kent','Rodney Matthews','Roger Ballen','Roger Dean','Ron Walotsky',\n           'Rossdraws','Ross Tran','Roz Chast','Salvador Dalí','Sam Spratt','Sandro Botticelli','Saul Steinberg','Saul Tepper',\n           'Seb McKinnon','Simon Bisley','Simon Stalenhag','Sir John Tenniel','Slawomir Maniak','Sonia Delaunay','sparth',\n           'Stephan Martiniere','Stevan Dohanos','Steve Dillon','Steven DaLuz','Studio Ghibli','Syd Mead','Sylvain Sarrailh',\n           'Takashi Murakami','Takato Yamamoto','Takeshi Obata','Tamara Lempicka','Taro Okamoto','Ted DeGrazia','Ted Nasmith',\n           'Terry Oakes','Terry Redlin','Thomas Cole','Thomas Kinkade','Thomas Nast','Thornton Oakley','Brothers Hildebrandt',\n           'Tim White','Titian','Tom Lovell','Tom Thomson','Tomek Setowski','Tomer Hanuka','Tomi Ungerer','Tomokazu Matsuyama',\n           'Tony Sart','Tsutomu Nihei','Tyler Edlin','Utagawa Kuniyoshi','Victo Ngai','Vincent Di Fate','Vladimir Kush',\n           'Wally Wood','Walter Beach Humphrey','Walter Crane','Warwick Goble','Wassily Kandinsky','Wayne Barlowe','Wendy Froud',\n           'Wifredo Lam','Will Eisner','William Hogarth','William Michael Harnett','William Steig','William Stout',\n           'William-Adolphe Bouguereau','Winslow Homer','Winsor McCay','WLOP','Yayoi Kusama','Yoshitaka Amano','Yue Minjun',\n           'Yves Tanguy','Zdzisław Beksiński']\n\njuice = ['dynamic composition','cinematic lighting','intricate','studio quality','highly detailed',\n          'digital painting', 'artstation', 'matte', 'sharp focus','hyper detailed', 'super sharp',\n          'crisp', 'smooth', 'smooth gradients', 'depth of field','insanely detailed and intricate',\n          'hypermaximalist', 'elegant', 'ornate', 'hyper realistic', 'super detailed', 'cinematic light',\n          'ray tracing', 'volumetric lighting', 'octane render','cinematic lighting', 'highly detailed',\n          'sharp focus', 'professional photoshoot', '8k', 'DOF','dramatically lit', '1ms shutter speed',\n          'back lighting', 'F 2.8 lens']\n\nstyle = ['2d game art','3D VR painting','8k resolution','1950s pulp sci-fi cover','anime','artistic photograph','Baroque painting','Byzantine mosaic','Chiaroscuro painting','depth of field','digital painting','dutch golden age','filmed in IMAX','fine art','flat shading','Flemish Baroque','Fresco painting','Gouache Painting','graffiti','Grisaille painting','highly detailed','hyperrealism','Impasto painting','low-poly','Luminism painting','Marvel Comics','matte painting','mixed media','oil painting','Panorama','parallax','pastel painting','pencil sketch','Perspective painting','Playstation 5 screenshot','pop art','raytracing','rendered in cinema4d','rendered in maya','rendered in zbrush','schematic','sculpture','Sfumato painting','shot on 70mm','Sotto In Su','storybook illustration','surrealist art','surveillance footage','Tempera Painting','tilt shift','Trompe L’oeil','Ukiyo-e','unreal engine render','vector image','Veduta painting','visionary hypermaximalism','volumetric lighting','vray tracing']\nsites = ['500px','ArtStation','Behance','cgsociety','ConceptArtWorld','DeviantArt','Flickr','Getty Images','Pixiv','unsplash','zbrushcentral']\ngenre = ['anime','art deco','antique lithograph','concept','cyberpunk','dark fantasy','enlightenment','fantasy','fauvism','film noir','gothic','holography','linocut','massurrealism','medieval','monochrome','oil painting','pencil sketch','photoreal','post-impressionist','postmodern','psychedelic','renaissance','sci-fi','steampunk','clean vector','victorian','vintage','woodblock']\n\nprompt_ideas_map = {'artists': artists,'juice': juice, 'style': style, 'sites': sites, 'genre': genre}\n\ndef get_random_style():\n  styles = []\n  for k, v in prompt_ideas_map.items():\n    if k == 'artists':\n      # only 1 artist\n      if random.random() > 0.1:\n        artist = random.choice(v)\n        styles.append(f'by {artist}')\n    else:\n      count = random.randint(0, 3)\n      if count > 0:\n        styles.extend(random.sample(v, k=count))\n  return ', '.join(styles)\n\nget_random_style()\n\n'by Chris Rallis, dynamic composition, Gouache Painting, pencil sketch, Behance, cgsociety, gothic, monochrome, antique lithograph'\n\n\nNow let’s try our prompt generator: (change the seed to get different prompts)\n\nrandom.seed(42)\nprompt = \"a photo of a woman wearing a red dress, perfect face, \"\n\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\ncount = 4\nprompts = [prompt + get_random_style() for _ in range(count)]\nprint('\\n'.join(prompts))\n\nimage = text2image(\n    pipe,\n    prompt=prompts,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=[negative_prompt] * count,\n    num_images_per_prompt=1)\n\ndisplay(image)\n\na photo of a woman wearing a red dress, perfect face, by Alex Ross, insanely detailed and intricate, depth of field, surveillance footage\na photo of a woman wearing a red dress, perfect face, by Alfred Kubin, cgsociety\na photo of a woman wearing a red dress, perfect face, by Syd Mead, depth of field, professional photoshoot, elegant, Flickr, fauvism, cyberpunk\na photo of a woman wearing a red dress, perfect face, by Tony Sart, artstation, digital painting, Baroque painting, Impasto painting, Veduta painting, unsplash, ConceptArtWorld\n\n\n\n\n\n\n\n\nNow that’s much much better! We haven’t changed anything other than just the positive and negative prompts. If you have really bad images, the first thing you would like to change is the prompts.\nFor more ideas of prompts, https://lexica.art/ is a good place to find what other people are using."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "href": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Refine your image using Image2Image",
    "text": "Refine your image using Image2Image\nThe image to image pipeline allows you to start from an initial image rather than a random one.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n    model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)\n\n\n\n\nMost of the Image2Image pipeline parameters should look similar, except:\n\ninit_image this is the image you start from, i.e. the image you want to fix.\nstrength and steps: strength is a number between 0-1, meaning how much your output depends on your init image. For example a 0.6 strength for 100 steps means doing diffusion starting from 40% of the init image, and then do 60 steps.\n\n\nimport PIL\nimage = PIL.Image.open('image_to_fix.jpg')\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 200\nstrength=0.6\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\nimages = pipe_i2i(\n    init_image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    strength=strength,\n    num_inference_steps=num_inference_steps,\n    generator=generator,\n    guidance_scale=guidance_scale,\n    num_images_per_prompt=num_images_per_prompt\n).images\n\ndisplay(image)\ndisplay(image_grid(images))\n\n\n\n\n\n\n\n\n\n\nNote that for the image2image, I used way more steps - since I want to improve the quality.\nNow comparing the generated images to the initial one, there are some images that are better. The second one is already quite good, while the first one is good except for the eyes. How can we fix that? Let’s first save it:\n\nimages[0].save('image_fix_eye.jpg')"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "href": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Fixing your image using Inpainting",
    "text": "Fixing your image using Inpainting\nThe in-painting pipeline allows you to mask a region of the image, and re-generate only that region. Thus this is useful for adding small fixes to our images.\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, StableDiffusionInpaintPipeline, EulerAncestralDiscreteScheduler\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-inpainting\" #@param [\"stabilityai/stable-diffusion-2-inpainting\", \"runwayml/stable-diffusion-inpainting\"] {type:\"string\"}\n\nif model_id == \"runwayml/stable-diffusion-inpainting\":\n  inp_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n      model_id,\n      revision=\"fp16\",\n      torch_dtype=torch.float16,\n      use_auth_token=True\n  ).to(\"cuda\")\nelif model_id == \"stabilityai/stable-diffusion-2-inpainting\":\n  inp_pipe = DiffusionPipeline.from_pretrained(\n        model_id,\n        revision=\"fp16\",\n        torch_dtype=torch.float16,\n        # scheduler=scheduler # TODO currently setting scheduler here messes up the end result. A bug in Diffusers🧨\n      ).to(\"cuda\")\n  inp_pipe.scheduler = DPMSolverMultistepScheduler.from_config(inp_pipe.scheduler.config)\n  inp_pipe.enable_attention_slicing()\n  # inp_pipe.enable_xformers_memory_efficient_attention()\n\ninp_pipe.scheduler = EulerAncestralDiscreteScheduler(\n    num_train_timesteps=1000,\n    beta_end=0.012,\n    beta_start=0.00085,\n    beta_schedule=\"linear\",\n)\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\nTo use the in-painting pipeline, there is only one new paramter: - mask_image this is a black-white image where white pixels will be repainted, and black pixels will be preserved.\nFor the purpose of fixing our image, we just use the same prompts for generating this image. To ease the generation of a mask, we can use the gradio Web UI.\n\nimport gradio as gr\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n\ndef inpaint(input_image):\n  image, mask = input_image['image'], input_image['mask']\n  image = image.resize((512,512))\n  mask = mask.resize((512,512))\n\n  print('image:')\n  display(image)\n  print('mask:')\n  display(mask)\n\n  images = inp_pipe(\n      prompt=prompt,\n      negative_prompt=negative_prompt,\n      image=image,\n      mask_image=mask.convert('RGB'),\n      width=512,\n      height=512,\n      generator=generator,\n      guidance_scale=guidance_scale,\n      num_inference_steps=num_inference_steps,\n      num_images_per_prompt=num_images_per_prompt,\n  ).images\n\n  result = image_grid(images)\n  print('result:')\n  display(result)\n  return result\n\nwith gr.Blocks() as demo:\n  gr.Markdown('In painting demo')\n\n  with gr.Row():\n    with gr.Column():\n      input_image = gr.Image(label='Input', type = 'pil', tool='sketch', height=1024)\n      button = gr.Button('Inpaint')\n    with gr.Column():\n      output_image = gr.Image(label='Output', height=1024)\n\n  button.click(inpaint, inputs=input_image, outputs=output_image)\n\ndemo.launch(debug=True, share=True)\n\nBy Default the in painting works terrible when fixing a small region:\n\n\n\n\n\n\n\n\nBut If we apply a small trick: zoom in on the area, then fix a larger portion, it will do a much better job:\n\n\n\n\n\n\n\n\nThus, if we can programmatically “zoom in”, fix the region, then paste to the original image, we can do a much better job. This is left to the reader as an interesting excercise."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "href": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Upsampling your image",
    "text": "Upsampling your image\nYou can boost our image resolution even higher by using the upsampler. Note this takes a lot of GPU memory.\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\nupscale_pipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16).to(\"cuda\")\n\nWARNING:root:WARNING: /usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\nNeed to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop\n\n\n/usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\n\nimport PIL\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = PIL.Image.open('low_res.png').convert('RGB').resize((128,128))\n\nupscale_image = upscale_pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator\n).images[0]\n\ndisplay(image)\ndisplay(upscale_image)"
  },
  {
    "objectID": "posts/python/python-parallel-processing.html",
    "href": "posts/python/python-parallel-processing.html",
    "title": "Python Parellel Processing",
    "section": "",
    "text": "I came across this function called parallel in fastai, and it seems very interesting.\n\n\n\n\n\n\nA Simple Example\n\nfrom fastcore.all import parallel\n\n\nfrom nbdev.showdoc import doc\n\n\ndoc(parallel)\n\nparallel[source]parallel(f, items, *args, n_workers=8, total=None, progress=None, pause=0, **kwargs)\n\nApplies func in parallel to items, using n_workers\nShow in docs\n\n\nAs the documentation states, the parallel function can run any python function f with items using multiple workers, and collect the results.\nLet’s try a simple examples:\n\nimport math\nimport time\n\ndef f(x):\n  time.sleep(1)\n  return x * 2\n\nnumbers = list(range(10))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 s\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 32 ms, sys: 52 ms, total: 84 ms\nWall time: 2.08 s\n\n\nThe function f we have in this example is very simple: it sleeps for one second and then returns x*2. When executed in serial, it takes 10 seconds which is exactly what we expect. When using more workers(8 by default), it takes only 2 seconds.\n\n\nDig into the Implementation\nLet’s see how parallel is implemented:\n\nparallel??\n\n\nSignature:\nparallel(\n    f,\n    items,\n    *args,\n    n_workers=8,\n    total=None,\n    progress=None,\n    pause=0,\n    **kwargs,\n)\nSource:   \ndef parallel(f, items, *args, n_workers=defaults.cpus, total=None, progress=None, pause=0, **kwargs):\n    \"Applies `func` in parallel to `items`, using `n_workers`\"\n    if progress is None: progress = progress_bar is not None\n    with ProcessPoolExecutor(n_workers, pause=pause) as ex:\n        r = ex.map(f,items, *args, **kwargs)\n        if progress:\n            if total is None: total = len(items)\n            r = progress_bar(r, total=total, leave=False)\n        return L(r)\nFile:      /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:      function\n\n\n\n\n\n??ProcessPoolExecutor\n\n\nInit signature:\nProcessPoolExecutor(\n    max_workers=8,\n    on_exc=<built-in function print>,\n    pause=0,\n    mp_context=None,\n    initializer=None,\n    initargs=(),\n)\nSource:        \nclass ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n    \"Same as Python's ProcessPoolExecutor, except can pass `max_workers==0` for serial execution\"\n    def __init__(self, max_workers=defaults.cpus, on_exc=print, pause=0, **kwargs):\n        if max_workers is None: max_workers=defaults.cpus\n        self.not_parallel = max_workers==0\n        store_attr(self, 'on_exc,pause,max_workers')\n        if self.not_parallel: max_workers=1\n        super().__init__(max_workers, **kwargs)\n    def map(self, f, items, *args, **kwargs):\n        self.lock = Manager().Lock()\n        g = partial(f, *args, **kwargs)\n        if self.not_parallel: return map(g, items)\n        try: return super().map(partial(_call, self.lock, self.pause, self.max_workers, g), items)\n        except Exception as e: self.on_exc(e)\nFile:           /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we can see in the source code, under the hood, this is using the concurrent.futures.ProcessPoolExecutor class from Python.\nNote that this class is essentially different than Python Threads, which is subject to the Global Interpreter Lock.\nThe ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned.\n\n\nUse cases\nThis function can be quite useful for long running tasks and you want to take advantage of multi-core CPUs to speed up your processing. For example, if you want to download a lot of images from the internet, you may want to use this to parallize your download jobs.\nIf your function f is very fast, there can be suprising cases, here is an example:\n\nimport math\nimport time\n\ndef f(x):\n  return x * 2\n\nnumbers = list(range(10000))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 1.24 ms\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 3.96 s, sys: 940 ms, total: 4.9 s\nWall time: 12.4 s\n\n\nIn the above example, f is very fast and the overhead of creating a lot of tasks outweigh the advantage of multi-processing. So use this with caution, and always take profiles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chi’s Blog",
    "section": "",
    "text": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPython Parellel Processing\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Chi is currently at Google, before which he has worked for Xiaomi, Amazon and PDD as a software engineer. His experiences include data and machine learning, especially in Ads and NLP."
  }
]