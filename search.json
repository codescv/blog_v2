[
  {
    "objectID": "posts/sd/stable-diffusion-2.html",
    "href": "posts/sd/stable-diffusion-2.html",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "",
    "text": "This is the second one of the Stable Diffusion tutorial series. In this tutorial, we will learn how to fine-tune the stable diffusion model on new images (aka Dreambooth)."
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#getting-images",
    "href": "posts/sd/stable-diffusion-2.html#getting-images",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Getting Images",
    "text": "Getting Images\nFor the purpose of this tutorial, let’s download some images from DuckDuckGo. Alternatively you can prepare your own images and put them inside a Google Drive Folder (referred to as images_src_dir in the next sections).\n\nimport pathlib\nfrom fastbook import search_images_ddg, download_images, verify_images, get_image_files\n\ndef download_images_for_keyword(keyword: str, download_dir: str, max_images):\n  dest = pathlib.Path(download_dir)\n  dest.mkdir(exist_ok=True)\n  results = search_images_ddg(keyword, max_images=max_images)\n  download_images(dest, urls=results)\n  downloaded_files = get_image_files(dest)\n  failed = verify_images(downloaded_files)\n  num_failed = len(failed)\n  if num_failed > 0:\n    print(f'Removing {num_failed} images')\n    failed.map(pathlib.Path.unlink)\n\nkeyword = 'Saito Asuka' #@param {type: \"string\"}\ndownload_dir = '/content/drive/MyDrive/sd/images_download' #@param {type: \"string\"}\nmax_images = 40 #@param {type: \"slider\", min:20, max:100, step:1}\nforce_redownload = False #@param {type:\"boolean\"}\n\nif not pathlib.Path(download_dir).exists() or force_redownload:\n  download_images_for_keyword(keyword=keyword, download_dir=download_dir, max_images=max_images)"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#cropping-and-resizing",
    "href": "posts/sd/stable-diffusion-2.html#cropping-and-resizing",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Cropping and Resizing",
    "text": "Cropping and Resizing\nAlthough SD doesn’t put restrictions on image sizes (other than the width and height should be divisible by 8), we preform center crop and resize on all images to make them the same square shape, since the training batches need to be same dimensions.\nCropping might result in bad images, but no worries, we will clean them up in the next section.\n\nimport PIL\nimport pathlib\nfrom fastai.vision.widgets import ImagesCleaner\nfrom fastai.vision.all import PILImage\nfrom fastbook import get_image_files\n\nimages_src_dir = '/content/drive/MyDrive/sd/images_download' #@param {type: \"string\"}\nimages_train_dir = '/content/drive/MyDrive/sd/images_training' #@param {type: \"string\"}\nsize = 768 #@param {type: \"slider\", min:256, max:768, step:128}\nimages_dir = pathlib.Path(images_train_dir)\nimages_dir.mkdir(exist_ok=True)\n\nfor idx, image_file in enumerate(get_image_files(images_src_dir)):\n  im = PILImage.create(image_file)\n  im = im.crop_pad(min(im.size)).resize_max(max_h=size)\n  im.save(images_dir / image_file.name)\n\n\n!ls \"{images_src_dir}\" |wc -l\n!ls \"{images_train_dir}\" |wc -l\n\n34\n34"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#cleaning-up-images",
    "href": "posts/sd/stable-diffusion-2.html#cleaning-up-images",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Cleaning Up Images",
    "text": "Cleaning Up Images\nOne of the most important things of any ML applications, if not THE most important - is the data quality. To get best quality, let’s check our training images and remove the “bad” ones (especially the ones that doesn’t contain complete faces after cropping - we don’t want the final model to learn to generate half faces!)\nFastAI provides an ImagesCleaner class which is a very cool tool for removing images from the Jupyter notebook. Just select “Delete” for the images you want to delete, and then run the following cells to delete them.\n\nfns = get_image_files(images_train_dir)\nw = ImagesCleaner(max_n=100)\nw.set_fns(fns)\nw\n\n\n\n\n\n\n\n\n\n\nw.fns[w.delete()].map(pathlib.Path.unlink)\n\n(#5) [None,None,None,None,None]\n\n\n\nfns = get_image_files(images_train_dir)\nw_clean = ImagesCleaner(max_n=100)\nw_clean.set_fns(fns)\nw_clean"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#using-the-diffusers-example-script",
    "href": "posts/sd/stable-diffusion-2.html#using-the-diffusers-example-script",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Using the diffusers example script",
    "text": "Using the diffusers example script\nFirst let’s train with the diffusers example script.\nThere are a few params worth noting:\n\nMODEL_NAME This is the id of your base model, e.g. SD1.5 or SD2.0. By default for training the SD2.0 with 768 resolution (“stabilityai/stable-diffusion-2”) you will need more than 16GB memory (about 22GB without xformers from my experiments).\nSTEPS The number of steps to train. Recommended value is ${num_examples} $.\nSAVE_STEPS The model will be exported every X steps, to avoid losing all progress when your GPU is recycled, or enable comparing different checkpoints to see which is best.\nOUTPUT_DIR This is where the trained model is exported.\nINSTANCE_DIR This points to the training images.\nINSTANCE_PROMPT This is the prompt for the training instances. In the diffusers example we are using a fixed prompt “a photo of xyz” for every instance image. This may not be optimal, and we’ll see how we can improve it later on.\n\nMake sure you selected GPU runtime (preferrably the best GPU you can choose) before continuing.\n\nMODEL_NAME=\"stabilityai/stable-diffusion-2\" #@param [\"runwayml/stable-diffusion-v1-5\", \"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\"]\nSTEPS=10000 #@param {type: \"integer\"}\nSAVE_STEPS=3000 #@param {type: \"integer\"}\nOUTPUT_DIR=\"/content/drive/MyDrive/sd/models/asuka\" #@param {type: \"string\"}\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\nINSTANCE_PROMPT=\"a photo of saitoasuka\" #@param {type: \"string\"}\nLEARNING_RATE=4e-6 #@param {type: \"number\"}\n\nif MODEL_NAME == \"stabilityai/stable-diffusion-2\":\n  resolution = 768\nelse:\n  resolution = 512\n\n!accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n  --train_text_encoder \\\n  --pretrained_model_name_or_path=\"{MODEL_NAME}\"  \\\n  --instance_data_dir=\"{INSTANCE_DIR}\" \\\n  --output_dir=\"{OUTPUT_DIR}\" \\\n  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n  --resolution={resolution} \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate={LEARNING_RATE} \\\n  --lr_scheduler=\"polynomial\" \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --lr_warmup_steps=0 \\\n  --save_steps={SAVE_STEPS} \\\n  --max_train_steps={STEPS}"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#using-blip-captions",
    "href": "posts/sd/stable-diffusion-2.html#using-blip-captions",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Using BLIP Captions",
    "text": "Using BLIP Captions\nIn the vanilla case, we used the same prompt for every image instance, which can be not ideal for the model - this is a training / testing mismatch. Because in training we have the same short prompt, but in validation we use very different prompts.\nBLIP is a model for generating text from images. We can use this model to automatically generate some informative descriptions and append to the prompts. This could help our model to generalize better.\n\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\n\nimport os\nimport sys\n\nif not os.path.exists('/content/BLIP'):\n  !rm -rf /content/BLIP\n  !git clone https://github.com/salesforce/BLIP.git\n  !pip install -Uqq timm fairscale pycocoevalcap\n\nif '/content/BLIP' not in sys.path:\n  sys.path.append('/content/BLIP')\n\nfrom models.blip import blip_decoder\nfrom torchvision import transforms\nimport PIL\nimport requests\nimport torch\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimage_size = 384\ntransform = transforms.Compose([\n  transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n  transforms.ToTensor(),\n  transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n])\n\nmodel_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\nmodel = blip_decoder(pretrained=model_url, med_config='/content/BLIP/configs/med_config.json', image_size=image_size, vit='large')\nmodel.eval()\nmodel = model.to(device)\n\ndef image2text(raw_image):\n  image = transform(raw_image).unsqueeze(0).to(device)\n  results = []\n  with torch.no_grad():\n    for _ in range(5):\n      caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n      results.extend(caption)\n  return results\n\nprint('generating blip captions')\nimage_captions = {}\nfor image_filename in os.listdir(INSTANCE_DIR):\n  image_filename_full = os.path.join(INSTANCE_DIR, image_filename)\n  image = PIL.Image.open(image_filename_full).convert('RGB')\n  text = image2text(image)\n  image_captions[image_filename_full] = text\n\nimport pickle\nwith open('image_captions.pickle', 'wb') as f:\n    pickle.dump(image_captions, f, protocol=pickle.HIGHEST_PROTOCOL)\nprint('finished generating blip captions')"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#training-1",
    "href": "posts/sd/stable-diffusion-2.html#training-1",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Training",
    "text": "Training\nWe are using the diffusers training scripts as a base, and make changes to the DreamboothDataset class so that it can now accept an additional parameter image_captions containing prompt mappings. For accelerate to run in Colab, we made some small changes, but those should be quite straightforward.\n\nPRETRAINED_MODEL = \"stabilityai/stable-diffusion-2\" #@param [\"runwayml/stable-diffusion-v1-5\", \"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\"] {allow-input: true, type: \"string\"}\nSTEPS=10000 #@param {type: \"integer\"}\nSAVE_STEPS=3000 #@param {type: \"integer\"}\nOUTPUT_DIR=\"/content/drive/MyDrive/sd/models/asuka_blip_v2\" #@param {type: \"string\"}\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\nINSTANCE_PROMPT=\"a photo of saitoasuka\" #@param {type: \"string\"}\nUSE_BLIP_CAPTIONS = True #@param {type:\"boolean\"}\nLEARNING_RATE=3e-6 #@param {type: \"number\"}\n\nfrom argparse import Namespace\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL\nimport random\nimport pickle\n\nargs = Namespace(\n    pretrained_model_name_or_path=PRETRAINED_MODEL, # Path to pretrained model or model identifier from huggingface.co/models.\n    revision=None,  # Revision of pretrained model identifier from huggingface.co/models.\n    tokenizer_name=None, # Pretrained tokenizer name or path if not the same as model_name\n    instance_data_dir=INSTANCE_DIR, # A folder containing the training data of instance images.\n    class_data_dir=None, # A folder containing the training data of class images.\n    instance_prompt=INSTANCE_PROMPT, # The prompt with identifier specifying the instance\n    class_prompt=None, # The prompt to specify images in the same class as provided instance images.\n    with_prior_preservation=False, # Flag to add prior preservation loss.\n    prior_loss_weight=1.0, # The weight of prior preservation loss.\n    num_class_images=100, # Minimal class images for prior preservation loss. If not have enough images, additional images will be sampled with class_prompt.\n    output_dir=OUTPUT_DIR, # The output directory where the model predictions and checkpoints will be written.\n    seed=None, # A seed for reproducible training.\n    resolution=768 if PRETRAINED_MODEL == \"stabilityai/stable-diffusion-2\" else 512, # The resolution for input images, all the images in the train/validation dataset will be resized to this resolution\n    center_crop=True, # Whether to center crop images before resizing to resolution\n    train_text_encoder=True, # Whether to train the text encoder\n    train_batch_size=1, # Batch size (per device) for the training dataloader.\n    sample_batch_size=4, # Batch size (per device) for sampling images.\n    num_train_epochs=1,\n    max_train_steps=STEPS, # Total number of training steps to perform.  If provided, overrides num_train_epochs.\n    save_steps=SAVE_STEPS, # Save checkpoint every X updates steps.\n    gradient_accumulation_steps=1, # Number of updates steps to accumulate before performing a backward/update pass.\n    gradient_checkpointing=True, # Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n    learning_rate=LEARNING_RATE, # Initial learning rate (after the potential warmup period) to use.\n    scale_lr=False, # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n    lr_scheduler=\"polynomial\", # The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n    lr_warmup_steps=0, # Number of steps for the warmup in the lr scheduler.\n    use_8bit_adam=True, # Whether or not to use 8-bit Adam from bitsandbytes\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_weight_decay=1e-2,\n    adam_epsilon=1e-8,\n    max_grad_norm=1.0,\n    push_to_hub=False,\n    hub_token=None,\n    hub_model_id=None,\n    logging_dir='logs',\n    mixed_precision=None, # [\"no\", \"fp16\", \"bf16\"]\n    local_rank=-1 # For distributed training: local_rank\n)\n\n\nif not os.path.exists('train_dreambooth.py'):\n  !cd /content/\n  !wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth.py -O train_dreambooth.py\n\nassert os.path.exists('train_dreambooth.py'), 'Unable to download train_dreambooth.py'\nimport train_dreambooth\ntrain_dreambooth.args = args\nfrom train_dreambooth import *\n\n\nif USE_BLIP_CAPTIONS:\n  with open('image_captions.pickle', 'rb') as f:\n    image_captions = pickle.load(f)\nelse:\n  image_captions = {}\nprint('Using image captions:', image_captions)\n\n\nclass DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        image_captions=None,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self.image_captions = image_captions or {}\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5]),\n            ]\n        )\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, index):\n        example = {}\n        image_path = self.instance_images_path[index % self.num_instance_images]\n        captions = self.image_captions.get(str(image_path))\n        if captions:\n          caption = random.choice(captions)\n          prompt = f'{self.instance_prompt}, {caption}'\n        else:\n          prompt = self.instance_prompt\n        instance_image = Image.open(image_path)\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example\n\ndef display_example(tokenizer, ex):\n  image = ex['instance_images']\n  image_norm = ((image+1) / 2 * 255)\n  image_np = image_norm.numpy().astype(np.uint8).transpose((1,2,0))\n  prompt = tokenizer.decode(ex['instance_prompt_ids'])\n  raw_image = PIL.Image.fromarray(image_np)\n  print(prompt)\n  display(raw_image)\n\ndef display_training_examples():\n  tokenizer = AutoTokenizer.from_pretrained(\n      PRETRAINED_MODEL,\n      subfolder=\"tokenizer\",\n      revision=None,\n      use_fast=False,\n  )\n\n  train_dataset = DreamBoothDataset(\n      instance_data_root=INSTANCE_DIR,\n      instance_prompt=INSTANCE_PROMPT,\n      image_captions=image_captions,\n      class_data_root=None,\n      class_prompt=None,\n      tokenizer=tokenizer,\n      size=512,\n      center_crop=False,\n  )\n  print(\"Display a few training examples\")\n  for idx, ex in enumerate(train_dataset):\n    display_example(tokenizer, ex)\n    if idx >= 3:\n      break\n\ndef main(args):\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=\"tensorboard\",\n        logging_dir=logging_dir,\n    )\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n            pipeline = DiffusionPipeline.from_pretrained(\n                args.pretrained_model_name_or_path,\n                torch_dtype=torch_dtype,\n                safety_checker=None,\n                revision=args.revision,\n            )\n            pipeline.set_progress_bar_config(disable=True)\n\n            num_new_images = args.num_class_images - cur_class_images\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\n            sample_dataloader = accelerator.prepare(sample_dataloader)\n            pipeline.to(accelerator.device)\n\n            for example in tqdm(\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n            ):\n                images = pipeline(example[\"prompt\"]).images\n\n                for i, image in enumerate(images):\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n                    image.save(image_filename)\n\n            del pipeline\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            repo = Repository(args.output_dir, clone_from=repo_name)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load the tokenizer\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name,\n            revision=args.revision,\n            use_fast=False,\n        )\n    elif args.pretrained_model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.pretrained_model_name_or_path,\n            subfolder=\"tokenizer\",\n            revision=args.revision,\n            use_fast=False,\n        )\n\n    # import correct text encoder class\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n\n    # Load models and create wrapper for stable diffusion\n    text_encoder = text_encoder_cls.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=args.revision,\n    )\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"vae\",\n        revision=args.revision,\n    )\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"unet\",\n        revision=args.revision,\n    )\n\n    vae.requires_grad_(False)\n    if not args.train_text_encoder:\n        text_encoder.requires_grad_(False)\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        if args.train_text_encoder:\n            text_encoder.gradient_checkpointing_enable()\n\n    if args.scale_lr:\n        args.learning_rate = (\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n        )\n\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n    if args.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n            )\n\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    params_to_optimize = (\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n    )\n    optimizer = optimizer_class(\n        params_to_optimize,\n        lr=args.learning_rate,\n        betas=(args.adam_beta1, args.adam_beta2),\n        weight_decay=args.adam_weight_decay,\n        eps=args.adam_epsilon,\n    )\n\n    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n\n    print('Image captions:', image_captions)\n    train_dataset = DreamBoothDataset(\n        image_captions=image_captions,\n        instance_data_root=args.instance_data_dir,\n        instance_prompt=args.instance_prompt,\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n        class_prompt=args.class_prompt,\n        tokenizer=tokenizer,\n        size=args.resolution,\n        center_crop=args.center_crop,\n    )\n\n    def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad(\n            {\"input_ids\": input_ids},\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        batch = {\n            \"input_ids\": input_ids,\n            \"pixel_values\": pixel_values,\n        }\n        return batch\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=1\n    )\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n\n    if args.train_text_encoder:\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n        )\n    else:\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, optimizer, train_dataloader, lr_scheduler\n        )\n\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    # Move text_encode and vae to gpu.\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n    # as these models are only used for inference, keeping weights in full precision is not required.\n    vae.to(accelerator.device, dtype=weight_dtype)\n    if not args.train_text_encoder:\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if accelerator.is_main_process:\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\n\n    # Train!\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(args.num_train_epochs):\n        unet.train()\n        if args.train_text_encoder:\n            text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n                timesteps = timesteps.long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                if args.with_prior_preservation:\n                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n                    target, target_prior = torch.chunk(target, 2, dim=0)\n\n                    # Compute instance loss\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n\n                    # Compute prior loss\n                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n\n                    # Add the prior loss to the instance loss.\n                    loss = loss + args.prior_loss_weight * prior_loss\n                else:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    params_to_clip = (\n                        itertools.chain(unet.parameters(), text_encoder.parameters())\n                        if args.train_text_encoder\n                        else unet.parameters()\n                    )\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                if global_step % args.save_steps == 0:\n                    if accelerator.is_main_process:\n                        pipeline = DiffusionPipeline.from_pretrained(\n                            args.pretrained_model_name_or_path,\n                            unet=accelerator.unwrap_model(unet),\n                            text_encoder=accelerator.unwrap_model(text_encoder),\n                            revision=args.revision,\n                        )\n                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                        pipeline.save_pretrained(save_path)\n\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = DiffusionPipeline.from_pretrained(\n            args.pretrained_model_name_or_path,\n            unet=accelerator.unwrap_model(unet),\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            revision=args.revision,\n        )\n        pipeline.save_pretrained(args.output_dir)\n\n        if args.push_to_hub:\n            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n\n    accelerator.end_training()\n\n# display_training_examples()\nimport accelerate\naccelerate.notebook_launcher(main, args=(args,))\n\n/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING:root:WARNING: /usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\nNeed to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop\n\n\n/usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\nUsing image captions: {'/content/drive/MyDrive/sd/images_training/be2c57cd-cc69-47f2-acbb-3cdb93a81b28.jpg': ['an asian woman in a white shirt posing for a picture', 'the woman is posing for the camera to show off her blue eyes', 'the young girl is posing for a picture', 'an asian woman with long hair, wearing a white shirt and denim overall', 'the young woman has long hair wearing white blouse'], '/content/drive/MyDrive/sd/images_training/335e70b8-85f8-4c7d-b4a5-c60428aef025.jpg': ['a woman wearing a black dress is leaning on a bed', 'a young girl laying on a bed while holding a controller', 'a woman in black and white dress holding her hand to a pole', 'an asian girl holding onto the arm of a white metal iron', 'a young woman posing with her hand on the side of a hand rail'], '/content/drive/MyDrive/sd/images_training/8a0205b5-7926-4753-8237-c40bb2838086.jpg': ['a pretty girl leaning on a wall looking to her left', 'a girl in blue shirt leaning against a yellow pole', 'a girl is staring out from a bus', 'a woman wearing a dress near a yellow wall', 'a young woman with bangs and eyes looking straight ahead'], '/content/drive/MyDrive/sd/images_training/0f369a7f-e18e-476a-a98a-f8ae901864ed.jpg': ['a girl with a leaf on her head', 'the young girl is posing with the bird on her head', 'a young girl posing in a wooded area holding her bird on the head', 'a little girl with a leaf on her head', 'a woman with a striped shirt and a green leaf on top of her head'], '/content/drive/MyDrive/sd/images_training/9d1ca46d-2071-474a-b8f5-7150dd9452da.jpg': ['an asian girl with blue eyes stares into the camera', 'a girl with very long hair and bangs', 'a girl in school uniform poses for a portrait', 'a close up of a person with a tie on', 'a girl with a tie is staring at the camera'], '/content/drive/MyDrive/sd/images_training/e4b74d9e-77ae-4161-8a1c-683623f4e963.jpg': ['a woman standing up against a purple background', 'the asian woman is posing for the photo', 'an asian woman poses for a portrait against a purple background', 'a young lady in black and white poses on a purple wall', 'an asian female in a black and white dress'], '/content/drive/MyDrive/sd/images_training/bf012971-ebb3-4007-a0ce-49a8a565b93a.jpg': ['woman holding wii controller in front of a camera', 'a woman with black hair, wearing white shirt', 'a woman is leaning on a rail holding a remote control', 'an attractive woman staring at the camera', 'the young asian girl is wearing a white t - shirt'], '/content/drive/MyDrive/sd/images_training/ffee321a-a9cb-4c05-a62e-59e832560f16.jpg': ['a girl with a messy ponytail standing against the wall', 'a beautiful young asian woman posing for a picture', 'the young woman has her eyes wide open and she is in white dress with flowers', 'a young asian woman in white is posing', 'a young asian girl in a white dress posing'], '/content/drive/MyDrive/sd/images_training/17ea5ae0-0e22-4945-8b42-f2a89dd90876.jpg': ['a girl with an oversized scarf is sitting on a bench', 'a woman sitting down and posing for the camera', 'a young woman is sitting down with a scarf', 'a beautiful asian woman wearing a grey scarf and red checkered pants', 'an asian woman sitting on a bench wearing a white scarf'], '/content/drive/MyDrive/sd/images_training/fcdf765b-72d4-4ffe-8305-7d9e48033fb1.jpg': ['a woman sitting at a table with her hands on her chin', 'a woman with long hair sitting with her chin resting on the table', 'a girl is sitting by the table and thinking', 'young girl wearing a white blouse with long hair', 'a woman with black hair and a long sleeved white shirt'], '/content/drive/MyDrive/sd/images_training/7c784480-cdfa-4fb1-88dc-cebefa1be0de.jpg': ['a beautiful young asian woman wearing a leopard print sweater', 'young girl with long straight hair leaning against white wall', 'a woman leaning against a white wall next to a wall', 'a woman leans against a wall with a leopard sweater on', 'a very pretty asian woman leaning against a wall'], '/content/drive/MyDrive/sd/images_training/019b20f8-cb84-4ef7-b4d9-b298856f4c81.jpg': ['a young girl sits in an airplane wearing a green hoodie', 'an asian girl in green jacket sitting at table with straw basket and baskets', 'a small child standing on a ledge above a valley', 'a young girl sitting on top of a chair', 'a little girl looks over at the camera'], '/content/drive/MyDrive/sd/images_training/0265b88b-e278-46bd-93db-5286efffb357.jpg': ['a woman in a santa hat and dress smiling', 'there is a girl that is wearing a hat on her head', 'a young woman in santa hat smiles for the camera', 'asian girl wearing a santa claus hat, smiling for the camera', 'asian woman wearing santa hat with holiday lights behind her'], '/content/drive/MyDrive/sd/images_training/15060cb6-b9f9-4099-89ed-52933b992e28.jpg': ['a young woman in white shirt holding a purple object', 'a woman that is standing next to a wooden table', 'young woman with long brown hair and a white shirt posing in front of dark wall', 'a young woman is wearing purple and white clothes', 'a girl with long brown hair wearing a white blouse'], '/content/drive/MyDrive/sd/images_training/d9c8300c-dca4-49f2-b38f-df09cf079100.jpg': ['a beautiful asian girl with dark hair posing for a picture', 'a woman sitting by a window wearing a pink coat', 'a close up of a person wearing a pink jacket and gloves', 'a girl with long black hair posing for the camera', 'a beautiful young asian woman sitting on a bench'], '/content/drive/MyDrive/sd/images_training/1e9388c5-067a-4dc9-84f1-29d05cb576ea.jpg': ['asian girl in pink dress brushing teeth with toothbrush', 'young woman with pink top brushing her teeth', 'a young woman is brushing her teeth in front of a mirror', 'a woman in a pink shirt is brushing her teeth', 'a young woman brushing her teeth in front of a mirror'], '/content/drive/MyDrive/sd/images_training/1287ea6d-83cb-44a4-8093-521580bd5271.jpg': ['woman with long dark hair posing for photo', 'a girl that has long brown hair', 'a beautiful woman with long brown hair sitting down', 'a woman looking to the right', 'woman with long black hair and grey shirt posing'], '/content/drive/MyDrive/sd/images_training/38bb0e3d-124b-470a-8727-b7a0c2922289.jpg': ['a girl brushes her teeth outside, with her hand', 'a close up of a person holding a brush in her mouth', 'the girl is holding a toothbrush in her mouth', 'the girl is eating a straw in the field', 'a girl with a spoon standing by herself'], '/content/drive/MyDrive/sd/images_training/763535ce-0c8b-4219-8fac-1fb153559a0b.jpg': ['a person wearing shorts sitting on a bed next to a window', 'an asian woman wearing blue and yellow looking at the camera', 'a very cute girl sitting in front of a window', 'a close up of a person in a dress near a window', 'a beautiful woman sitting in front of a window'], '/content/drive/MyDrive/sd/images_training/39ed29af-0c2e-4e12-9251-5938abddede3.jpg': ['a person with brown hair covering their face behind a fence', 'a close up of a woman holding her hands over her mouth', 'the woman has a hooded sweatshirt around her neck', 'a girl hiding her mouth under a blue jacket', 'there is a woman that is covering her face with a jacket'], '/content/drive/MyDrive/sd/images_training/7040c264-3016-4fab-9073-a1ea3b402867.png': ['a woman with long black hair looking at the camera', 'the girl in white shirt has her hair long', 'an asian woman is standing in front of a car', 'young japanese woman with long brown hair near a car', 'a young girl with long hair and bangs'], '/content/drive/MyDrive/sd/images_training/5b683650-3d6e-4b0b-bce7-928c09733ad8.jpg': ['the woman is posing for a photo while sitting', 'an asian woman sitting on a white couch', 'a young woman laying on top of a bed next to a pillow', 'asian girl with brown hair and bangs laying on the floor with hands clasped up', 'the girl in the white shirt is sitting by her bed'], '/content/drive/MyDrive/sd/images_training/a562af99-a96b-4425-adf1-0d8215c0df4d.jpg': ['a girl in a knitted hat and shirt', 'a woman wearing a grey beanie holding her hands under her hair', 'a young woman wearing a hat and staring directly', 'asian woman wearing a grey beanie with black trim', 'woman in white shirt putting on hat with hand'], '/content/drive/MyDrive/sd/images_training/be2fd2fd-cb1f-4415-8aa1-85fc17cebbc9.jpg': ['a girl with long dark hair laying on top of a sofa', 'the woman has dark hair and bangs on her head', 'a young asian lady with dark hair', 'the asian woman is wearing an oversized vest', 'woman holding up a blue cloth and wearing a gray shirt'], '/content/drive/MyDrive/sd/images_training/3133ea61-cf12-401b-98cb-9378d53dbb80.jpg': ['a beautiful young asian woman standing next to a wall', 'there is a young asian woman posing for the camera', 'asian girl standing by a white wall', 'a young asian lady is standing near a wall', 'a young woman leaning against a wall and looking off into the distance'], '/content/drive/MyDrive/sd/images_training/5ade1dbb-9ed5-49a7-9d1b-e3be262bd820.jpg': ['a woman that is wearing some kind of white shirt', 'a girl with a dark hair poses on the beach', 'woman with black hair in front of an ocean', 'a woman with long hair is staring away', 'young woman wearing t - shirt looking away from camera'], '/content/drive/MyDrive/sd/images_training/fa17157a-280a-4d7e-a8d8-bb7b115e5575.jpg': ['a young woman has long brown hair and bangs', 'a very pretty woman with a neat look', 'the woman with a long hair wearing a white shirt', 'the head of a woman with long brown hair', 'a young lady with long hair and big bangs'], '/content/drive/MyDrive/sd/images_training/9e967a0e-9cad-40a1-8791-57baca670e13.jpg': ['a pretty young woman holding a teddy bear', 'a woman holding a teddy bear while smiling', 'a beautiful young lady holding a stuffed teddy bear', 'asian woman in white dress holding a cream colored teddy bear', 'young woman holding a teddy bear while posing for picture'], '/content/drive/MyDrive/sd/images_training/3a1cc96c-2cb2-4430-b661-74650d8b351f.jpg': ['a girl poses in the grass near an oat plant', 'the girl is staring at the camera', 'young girl in blue shirt with dark hair and fringes', 'a young asian girl with long brown hair', 'a young woman in blue shirt posing for a picture']}\nLaunching training on one GPU.\n\n\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n\n\nImage captions: {'/content/drive/MyDrive/sd/images_training/be2c57cd-cc69-47f2-acbb-3cdb93a81b28.jpg': ['an asian woman in a white shirt posing for a picture', 'the woman is posing for the camera to show off her blue eyes', 'the young girl is posing for a picture', 'an asian woman with long hair, wearing a white shirt and denim overall', 'the young woman has long hair wearing white blouse'], '/content/drive/MyDrive/sd/images_training/335e70b8-85f8-4c7d-b4a5-c60428aef025.jpg': ['a woman wearing a black dress is leaning on a bed', 'a young girl laying on a bed while holding a controller', 'a woman in black and white dress holding her hand to a pole', 'an asian girl holding onto the arm of a white metal iron', 'a young woman posing with her hand on the side of a hand rail'], '/content/drive/MyDrive/sd/images_training/8a0205b5-7926-4753-8237-c40bb2838086.jpg': ['a pretty girl leaning on a wall looking to her left', 'a girl in blue shirt leaning against a yellow pole', 'a girl is staring out from a bus', 'a woman wearing a dress near a yellow wall', 'a young woman with bangs and eyes looking straight ahead'], '/content/drive/MyDrive/sd/images_training/0f369a7f-e18e-476a-a98a-f8ae901864ed.jpg': ['a girl with a leaf on her head', 'the young girl is posing with the bird on her head', 'a young girl posing in a wooded area holding her bird on the head', 'a little girl with a leaf on her head', 'a woman with a striped shirt and a green leaf on top of her head'], '/content/drive/MyDrive/sd/images_training/9d1ca46d-2071-474a-b8f5-7150dd9452da.jpg': ['an asian girl with blue eyes stares into the camera', 'a girl with very long hair and bangs', 'a girl in school uniform poses for a portrait', 'a close up of a person with a tie on', 'a girl with a tie is staring at the camera'], '/content/drive/MyDrive/sd/images_training/e4b74d9e-77ae-4161-8a1c-683623f4e963.jpg': ['a woman standing up against a purple background', 'the asian woman is posing for the photo', 'an asian woman poses for a portrait against a purple background', 'a young lady in black and white poses on a purple wall', 'an asian female in a black and white dress'], '/content/drive/MyDrive/sd/images_training/bf012971-ebb3-4007-a0ce-49a8a565b93a.jpg': ['woman holding wii controller in front of a camera', 'a woman with black hair, wearing white shirt', 'a woman is leaning on a rail holding a remote control', 'an attractive woman staring at the camera', 'the young asian girl is wearing a white t - shirt'], '/content/drive/MyDrive/sd/images_training/ffee321a-a9cb-4c05-a62e-59e832560f16.jpg': ['a girl with a messy ponytail standing against the wall', 'a beautiful young asian woman posing for a picture', 'the young woman has her eyes wide open and she is in white dress with flowers', 'a young asian woman in white is posing', 'a young asian girl in a white dress posing'], '/content/drive/MyDrive/sd/images_training/17ea5ae0-0e22-4945-8b42-f2a89dd90876.jpg': ['a girl with an oversized scarf is sitting on a bench', 'a woman sitting down and posing for the camera', 'a young woman is sitting down with a scarf', 'a beautiful asian woman wearing a grey scarf and red checkered pants', 'an asian woman sitting on a bench wearing a white scarf'], '/content/drive/MyDrive/sd/images_training/fcdf765b-72d4-4ffe-8305-7d9e48033fb1.jpg': ['a woman sitting at a table with her hands on her chin', 'a woman with long hair sitting with her chin resting on the table', 'a girl is sitting by the table and thinking', 'young girl wearing a white blouse with long hair', 'a woman with black hair and a long sleeved white shirt'], '/content/drive/MyDrive/sd/images_training/7c784480-cdfa-4fb1-88dc-cebefa1be0de.jpg': ['a beautiful young asian woman wearing a leopard print sweater', 'young girl with long straight hair leaning against white wall', 'a woman leaning against a white wall next to a wall', 'a woman leans against a wall with a leopard sweater on', 'a very pretty asian woman leaning against a wall'], '/content/drive/MyDrive/sd/images_training/019b20f8-cb84-4ef7-b4d9-b298856f4c81.jpg': ['a young girl sits in an airplane wearing a green hoodie', 'an asian girl in green jacket sitting at table with straw basket and baskets', 'a small child standing on a ledge above a valley', 'a young girl sitting on top of a chair', 'a little girl looks over at the camera'], '/content/drive/MyDrive/sd/images_training/0265b88b-e278-46bd-93db-5286efffb357.jpg': ['a woman in a santa hat and dress smiling', 'there is a girl that is wearing a hat on her head', 'a young woman in santa hat smiles for the camera', 'asian girl wearing a santa claus hat, smiling for the camera', 'asian woman wearing santa hat with holiday lights behind her'], '/content/drive/MyDrive/sd/images_training/15060cb6-b9f9-4099-89ed-52933b992e28.jpg': ['a young woman in white shirt holding a purple object', 'a woman that is standing next to a wooden table', 'young woman with long brown hair and a white shirt posing in front of dark wall', 'a young woman is wearing purple and white clothes', 'a girl with long brown hair wearing a white blouse'], '/content/drive/MyDrive/sd/images_training/d9c8300c-dca4-49f2-b38f-df09cf079100.jpg': ['a beautiful asian girl with dark hair posing for a picture', 'a woman sitting by a window wearing a pink coat', 'a close up of a person wearing a pink jacket and gloves', 'a girl with long black hair posing for the camera', 'a beautiful young asian woman sitting on a bench'], '/content/drive/MyDrive/sd/images_training/1e9388c5-067a-4dc9-84f1-29d05cb576ea.jpg': ['asian girl in pink dress brushing teeth with toothbrush', 'young woman with pink top brushing her teeth', 'a young woman is brushing her teeth in front of a mirror', 'a woman in a pink shirt is brushing her teeth', 'a young woman brushing her teeth in front of a mirror'], '/content/drive/MyDrive/sd/images_training/1287ea6d-83cb-44a4-8093-521580bd5271.jpg': ['woman with long dark hair posing for photo', 'a girl that has long brown hair', 'a beautiful woman with long brown hair sitting down', 'a woman looking to the right', 'woman with long black hair and grey shirt posing'], '/content/drive/MyDrive/sd/images_training/38bb0e3d-124b-470a-8727-b7a0c2922289.jpg': ['a girl brushes her teeth outside, with her hand', 'a close up of a person holding a brush in her mouth', 'the girl is holding a toothbrush in her mouth', 'the girl is eating a straw in the field', 'a girl with a spoon standing by herself'], '/content/drive/MyDrive/sd/images_training/763535ce-0c8b-4219-8fac-1fb153559a0b.jpg': ['a person wearing shorts sitting on a bed next to a window', 'an asian woman wearing blue and yellow looking at the camera', 'a very cute girl sitting in front of a window', 'a close up of a person in a dress near a window', 'a beautiful woman sitting in front of a window'], '/content/drive/MyDrive/sd/images_training/39ed29af-0c2e-4e12-9251-5938abddede3.jpg': ['a person with brown hair covering their face behind a fence', 'a close up of a woman holding her hands over her mouth', 'the woman has a hooded sweatshirt around her neck', 'a girl hiding her mouth under a blue jacket', 'there is a woman that is covering her face with a jacket'], '/content/drive/MyDrive/sd/images_training/7040c264-3016-4fab-9073-a1ea3b402867.png': ['a woman with long black hair looking at the camera', 'the girl in white shirt has her hair long', 'an asian woman is standing in front of a car', 'young japanese woman with long brown hair near a car', 'a young girl with long hair and bangs'], '/content/drive/MyDrive/sd/images_training/5b683650-3d6e-4b0b-bce7-928c09733ad8.jpg': ['the woman is posing for a photo while sitting', 'an asian woman sitting on a white couch', 'a young woman laying on top of a bed next to a pillow', 'asian girl with brown hair and bangs laying on the floor with hands clasped up', 'the girl in the white shirt is sitting by her bed'], '/content/drive/MyDrive/sd/images_training/a562af99-a96b-4425-adf1-0d8215c0df4d.jpg': ['a girl in a knitted hat and shirt', 'a woman wearing a grey beanie holding her hands under her hair', 'a young woman wearing a hat and staring directly', 'asian woman wearing a grey beanie with black trim', 'woman in white shirt putting on hat with hand'], '/content/drive/MyDrive/sd/images_training/be2fd2fd-cb1f-4415-8aa1-85fc17cebbc9.jpg': ['a girl with long dark hair laying on top of a sofa', 'the woman has dark hair and bangs on her head', 'a young asian lady with dark hair', 'the asian woman is wearing an oversized vest', 'woman holding up a blue cloth and wearing a gray shirt'], '/content/drive/MyDrive/sd/images_training/3133ea61-cf12-401b-98cb-9378d53dbb80.jpg': ['a beautiful young asian woman standing next to a wall', 'there is a young asian woman posing for the camera', 'asian girl standing by a white wall', 'a young asian lady is standing near a wall', 'a young woman leaning against a wall and looking off into the distance'], '/content/drive/MyDrive/sd/images_training/5ade1dbb-9ed5-49a7-9d1b-e3be262bd820.jpg': ['a woman that is wearing some kind of white shirt', 'a girl with a dark hair poses on the beach', 'woman with black hair in front of an ocean', 'a woman with long hair is staring away', 'young woman wearing t - shirt looking away from camera'], '/content/drive/MyDrive/sd/images_training/fa17157a-280a-4d7e-a8d8-bb7b115e5575.jpg': ['a young woman has long brown hair and bangs', 'a very pretty woman with a neat look', 'the woman with a long hair wearing a white shirt', 'the head of a woman with long brown hair', 'a young lady with long hair and big bangs'], '/content/drive/MyDrive/sd/images_training/9e967a0e-9cad-40a1-8791-57baca670e13.jpg': ['a pretty young woman holding a teddy bear', 'a woman holding a teddy bear while smiling', 'a beautiful young lady holding a stuffed teddy bear', 'asian woman in white dress holding a cream colored teddy bear', 'young woman holding a teddy bear while posing for picture'], '/content/drive/MyDrive/sd/images_training/3a1cc96c-2cb2-4430-b661-74650d8b351f.jpg': ['a girl poses in the grass near an oat plant', 'the girl is staring at the camera', 'young girl in blue shirt with dark hair and fringes', 'a young asian girl with long brown hair', 'a young woman in blue shirt posing for a picture']}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are the results using same settings:\n\n\n\n\n\n\n\n\n\n\nYou can see from the above result that the model is now respecting the “wearing a hat” prompt."
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#optional-xformers",
    "href": "posts/sd/stable-diffusion-2.html#optional-xformers",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "[Optional] xformers",
    "text": "[Optional] xformers\nxformers allows memory efficient attention to save GPU memory. If not working, maybe run pip install -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers (takes a lot of time!)\n\nimport os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nimport xformers\nimport torch\n\ntry:\n  xformers.ops.memory_efficient_attention(\n      torch.randn((1, 2, 40), device=\"cuda\"),\n      torch.randn((1, 2, 40), device=\"cuda\"),\n      torch.randn((1, 2, 40), device=\"cuda\"),\n  )\n  print('xformers working')\nexcept:\n  print('xformers not working')\n  raise"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#setup",
    "href": "posts/sd/stable-diffusion-1.html#setup",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Setup",
    "text": "Setup\nTo begin our journey, first we need to install some packages.\n\ndiffusers This is the package that we are mainly using for generating images.\ntransformers This is the package to encode text into embeddings.\nAnd a few other supporting packages to work with the above packages.\n\n\n!pip install -Uqq diffusers transformers ftfy scipy accelerate gradio xformers triton==2.0.0.dev20221120\nimport pathlib\nimport huggingface_hub\nif not pathlib.Path('/root/.huggingface/token').exists():\n  huggingface_hub.notebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#utility-functions",
    "href": "posts/sd/stable-diffusion-1.html#utility-functions",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Utility Functions",
    "text": "Utility Functions\n\nimport PIL\nimport math\n\ndef image_grid(imgs, rows=None, cols=None) -> PIL.Image.Image:\n  n_images = len(imgs)\n  if not rows and not cols:\n    cols = math.ceil(math.sqrt(n_images))\n  if not rows:\n    rows = math.ceil(n_images / cols)\n  if not cols:\n    cols = math.ceil(n_images / rows)\n\n  w, h = imgs[0].size\n  grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n\n  for i, img in enumerate(imgs):\n      grid.paste(img, box=(i%cols*w, i//cols*h))\n  return grid"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Initializing the Pipeline",
    "text": "Initializing the Pipeline\nThere are two important params to intialize a Pipeline. a model_id and a revision.\n\nThe model_id can be either a huggingface model id or some path in your file system. To find which ones to use, go to huggingface and look for model id at the top. Later when we train our own models we will pass in the path to our trained model.\nThe revision can be set fp16 to save GPU memory by using 16 bit numbers.\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Generating image using the text to image pipeline",
    "text": "Generating image using the text to image pipeline\nTo generate an image from text, we just call the pipe() method. There are a few arguments to the method:\n\nprompt A text describing the thing you want the image to be.\nnegative_prompt A text describing the features that you don’t want the image to have.\ngenerator This is a random number generator. By default the generator is None, and the output is random for same prompts. A fixed seed allows repeatable experiment (same input-> same output).\nwidth and height the dimension of output image.\nguidance_scale A scale determining the extent of how your image accurately matches your prompt. In practice I find it not very useful to tune this, just use a 7.5 you will be fine.\nnum_inference_steps How many steps to run the diffusion algorithm(will explain later). The more steps, the better the image quality, and the more time it takes to generate an image.\n\n\ndef text2image(pipe,\n               prompt: str,\n               seed: int,\n               return_grid=False,\n               grid_size=None,\n               **kwargs):\n  generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n  with torch.autocast(\"cuda\"):\n    images = pipe(prompt,\n                  generator=generator,\n                  **kwargs).images\n\n  if len(images) == 1:\n    return images[0]\n  elif return_grid:\n    return image_grid(images)\n  else:\n    return images"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#baseline",
    "href": "posts/sd/stable-diffusion-1.html#baseline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Baseline",
    "text": "Baseline\nFirst let’s try using a prompt with all default parameters:\n\nprompt = \"a photo of a woman wearing a red dress\"\nnegative_prompt = \"\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s terrible! If you get completely unusable images like the above, the first thing to change is the prompts."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "href": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Add Prompts and Negative Prompts",
    "text": "Add Prompts and Negative Prompts\nLet’s add a few style keywords to the prompt, and then some negative keywords:\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s far from perfect, but much better than our first try. Generally speaking stable diffusion version 2 works much worse than version 1.5 by default, and it needs much more effort in tuning the prompts.\nThe negative prompt usually works well for many different images, but for the positive prompts there can be a lot of possiblities.\nNow let’s build a systematic way of finding prompt additions:"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "href": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "A random prompt generator",
    "text": "A random prompt generator\n\nimport random\n\nartists = ['Aaron Douglas','Agnes Lawrence Pelton','Akihiko Yoshida','Albert Bierstadt','Albert Bierstadt','Alberto Giacometti',\n           'Alberto Vargas','Albrecht Dürer','Aleksi Briclot','Alex Grey','Alex Horley-Orlandelli','Alex Katz','Alex Ross',\n           'Alex Toth','Alexander Jansson','Alfons Maria Mucha','Alfred Kubin','Alphonse Mucha','Anato Finnstark','Anders Zorn',\n           'André Masson','Andreas Rocha','Andrew Wyeth','Anish Kapoor','Anna Dittmann','Anna Mary Robertson Moses','Anni Albers',\n           'Ansel Adams','Anthony van Dyck','Anton Otto Fischer','Antonio Mancini','April Gornik','Arnold Böcklin','Art Spiegelman',\n           'Artemisia Gentileschi','Artgerm','Arthur Garfield Dove','Arthur Rackham','Asher Brown Durand','Aubrey Beardsley',\n           'Austin Briggs','Ayami Kojima','Bastien Lecouffe-Deharme','Bayard Wu','Beatrix Potter','Beeple','Beksinski',\n           'Bill Sienkiewicz','Bill Ward','Bill Watterson','Bob Eggleton','Boris Vallejo','Brian Bolland','Brian Froud',\n           'Bruce Pennington','Bunny Yeager','Camille Corot','Camille Pissarro','Canaletto','Caravaggio','Caspar David Friedrich',\n           'Cedric Peyravernay','Charles Addams','Charles Dana Gibson','Chesley Bonestell','Chris Foss','Chris Moore',\n           'Chris Rallis','Chriss Foss','Cindy Sherman','Clara Peeters','Claude Monet','Clyde Caldwell','Coles Phillips',\n           'Cornelis Bisschop','Coby Whitmore','Craig Mullins','Cynthia Sheppard','Dale Chihuly','Damien Hirst','Dan Mumford',\n           'Daniel Merriam','Darek Zabrocki','Dave Dorman','Dave Gibbons','Dave McKean','David Firth','Dean Cornwell','Dean Ellis',\n           'Diane Dillon','Disney','Don Maitz','Donato Giancola','Dorothea Tanning','Dreamworks','Dr. Seuss','Earl Norem',\n           'Earle Bergey','Earnst Haeckel','Ed Emshwiller','Edgar Degas','Edmund Dulac','Edmund Leighton','Édouard Manet',\n           'Edvard Munch','Edward Burne-Jones','Edward Gorey','Edward Hopper','Edward Lear','Edwin Austin Abbey','Edwin Deakin',\n           'Egon Schiele','El Greco','Elizabeth Shippen Green','Emmanuel Shiu','Emory Douglas','Esao Andrews','Eugène Delacroix',\n           'Evelyn De Morgan','E.H. Shepard','F. Scott Hess','Fairfield Porter','Federico Pelat','Filippino Lippi','Fitz Henry Lane',\n           'Francis Bacon','Francisco Goya','Frank Frazetta','Frank Xavier Leyendecker','Franklin Booth','Franz Sedlacek',\n           'Frederick Edwin Church','Frederick McCubbin','Gaston Bussière','Gediminas Pranckevicius','Geof Darrow',\n           'George B. Bridgman','George Cruikshank','George Inness','George Luks',\"Georgia O'Keeffe\",'Gerald Brom','Giacomo Balla',\n           'Gil Elvgren','Gillis Rombouts','Giorgio de Chirico','Giorgione','Giovanni Battista Piranesi','Greg Hildebrandt',\n           'Greg Rutkowski','Greg Staples','Gregory Manchess','Guido Borelli da Caluso','Gustaf Tenggren','Gustav Klimt',\n           'Gustave Doré','Gustave Moreau','Gwen John','Hannah Höch','Hans Baldung','Hans Bellmer','Harrison Fisher','Harvey Dunn',\n           'Harvey Kurtzman','Henri de Toulouse-Lautrec','Henri Matisse','Henri Rousseau','Henry Ossawa Tanner','Henry Raleigh',\n           'Hethe Srodawa','Hieronymus Bosch','Hiromu Arakawa','Hokusai','Howard Chandler Christy','Howard Pyle','Hubert Robert',\n           'Hugh Ferriss','Hyun Lee','H.R. Giger','Igor Kieryluk','Igor Morski','Igor Wolkski','Ilya Kuvshinov','Ilya Repin',\n           'Inyi Han','Isaac Levitan','Ivan Aivazovsky','Ivan Albright','Ivan Bilibin','Ivan Shishkin','Jacek Yerka','Jack Kirby',\n           'Jackson Pollock','Jakub Rozalski','James C. Christensen','James Gillray','James Gurney','James Jean','James Paick',\n           'Jamie Hewlett','Jan van Eyck','Janet Fish','Jasper Johns','J.C. Leyendecker','Jean Delville','Jean Giraud',\n           'Jean Metzinger','Jean-Honoré Fragonard','Jean-Michel Basquiat','Jeff Easley','Jeff Koons','Jeffrey Smith',\n           'Jerome Lacoste','Jerry Pinkney','Jesper Ejsing','Jessie Willcox Smith','Jim Burns','Jim Steranko','Joaquín Sorolla',\n           'Joe Jusko','Johannes Vermeer','Johfra Bosschart','John Atkinson Grimshaw','John Bauer','John Berkey','John Constable',\n           'John Frederick Kensett','John French Sloan','John Harris','John Howe','John James Audubon','John Martin',\n           'John Philip Falter','John Romita Jr','Jon Foster','Jon Whitcomb','Joseph Cornell','Juan Gris','Junji Ito',\n           'J.M.W. Turner','Kadir Nelson','Kandinsky','Karol Bak','Kate Greenaway','Kawanabe Kyōsai','Kay Nielsen',\n           'Keith Haring','Kelly Freas','Kelly Mckernan','Kim Jung Gi','Kinuko Craft','Konstantin Vasilyev',\n           'Konstantinas Ciurlionis','Krenz Cushart','Lale Westvind','Larry Elmore','Laura Muntz Lyall','Laurel Burch',\n           'Laurie Lipton','Lawren Harris','Lee Madgwick','Leo and Diane Dillon','Leonora Carrington','Liam Wong','Lise Deharme',\n           'Lois van Baarle','Louis Glackens','Louis Janmot','Louise Bourgeois','Lucian Freud','Luis Royo','Lynda Benglis',\n           'Lyubov Popova','Maciej Kuciara','Makoto Shinkai','Malevich','Marc Simonetti','Margaret Macdonald Mackintosh',\n           'Maria Sibylla Merian','Marianne North','Mario Sanchez Nevado','Mark Ryden','Martin Johnson Heade','Mary Cassatt',\n           'Mati Klarwein','Maxfield Parrish','Mead Schaeffer','Michael Hussar','Michael Parkes','Michael Whelan',\n           'Mikalojus Konstantinas Čiurlionis','Mike Mignola','Milton Caniff','Milton Glaser','Moebius','Mondrian','M.C. Escher',\n           'Noah Bradley','Noriyoshi Ohrai','Norman Rockwell','N.C. Wyeth','Odd Nerdrum','Odilon Redon','Ohara Koson',\n           'Paul Cézanne','Paul Delvaux','Paul Gauguin','Paul Klee','Paul Lehr','Peter Elson','Peter Gric','Peter Helck',\n           'Peter Max','Peter Mohrbacher','Peter Paul Rubens','Pierre Bonnard','Pierre-Auguste Renoir','Pieter Bruegel the Elder',\n           'Pieter Claesz','Pixar','P.A. Works','Rafal Olbinski','Ralph Horsley','Ralph McQuarrie','Randolph Caldecott',\n           'Raphael Lacoste','Ray Caesar','Raymond Swanland','Rebecca Guay','Rembrandt','Rembrandt van Rijn','Rene Magritte',\n           'RHADS','Richard Dadd','Richter','Rob Gonsalves','Robert Delaunay','Robert McCall','Robert McGinnis',\n           'Robert Rauschenberg','Roberto da Matta','Rockwell Kent','Rodney Matthews','Roger Ballen','Roger Dean','Ron Walotsky',\n           'Rossdraws','Ross Tran','Roz Chast','Salvador Dalí','Sam Spratt','Sandro Botticelli','Saul Steinberg','Saul Tepper',\n           'Seb McKinnon','Simon Bisley','Simon Stalenhag','Sir John Tenniel','Slawomir Maniak','Sonia Delaunay','sparth',\n           'Stephan Martiniere','Stevan Dohanos','Steve Dillon','Steven DaLuz','Studio Ghibli','Syd Mead','Sylvain Sarrailh',\n           'Takashi Murakami','Takato Yamamoto','Takeshi Obata','Tamara Lempicka','Taro Okamoto','Ted DeGrazia','Ted Nasmith',\n           'Terry Oakes','Terry Redlin','Thomas Cole','Thomas Kinkade','Thomas Nast','Thornton Oakley','Brothers Hildebrandt',\n           'Tim White','Titian','Tom Lovell','Tom Thomson','Tomek Setowski','Tomer Hanuka','Tomi Ungerer','Tomokazu Matsuyama',\n           'Tony Sart','Tsutomu Nihei','Tyler Edlin','Utagawa Kuniyoshi','Victo Ngai','Vincent Di Fate','Vladimir Kush',\n           'Wally Wood','Walter Beach Humphrey','Walter Crane','Warwick Goble','Wassily Kandinsky','Wayne Barlowe','Wendy Froud',\n           'Wifredo Lam','Will Eisner','William Hogarth','William Michael Harnett','William Steig','William Stout',\n           'William-Adolphe Bouguereau','Winslow Homer','Winsor McCay','WLOP','Yayoi Kusama','Yoshitaka Amano','Yue Minjun',\n           'Yves Tanguy','Zdzisław Beksiński']\n\njuice = ['dynamic composition','cinematic lighting','intricate','studio quality','highly detailed',\n          'digital painting', 'artstation', 'matte', 'sharp focus','hyper detailed', 'super sharp',\n          'crisp', 'smooth', 'smooth gradients', 'depth of field','insanely detailed and intricate',\n          'hypermaximalist', 'elegant', 'ornate', 'hyper realistic', 'super detailed', 'cinematic light',\n          'ray tracing', 'volumetric lighting', 'octane render','cinematic lighting', 'highly detailed',\n          'sharp focus', 'professional photoshoot', '8k', 'DOF','dramatically lit', '1ms shutter speed',\n          'back lighting', 'F 2.8 lens']\n\nstyle = ['2d game art','3D VR painting','8k resolution','1950s pulp sci-fi cover','anime','artistic photograph','Baroque painting','Byzantine mosaic','Chiaroscuro painting','depth of field','digital painting','dutch golden age','filmed in IMAX','fine art','flat shading','Flemish Baroque','Fresco painting','Gouache Painting','graffiti','Grisaille painting','highly detailed','hyperrealism','Impasto painting','low-poly','Luminism painting','Marvel Comics','matte painting','mixed media','oil painting','Panorama','parallax','pastel painting','pencil sketch','Perspective painting','Playstation 5 screenshot','pop art','raytracing','rendered in cinema4d','rendered in maya','rendered in zbrush','schematic','sculpture','Sfumato painting','shot on 70mm','Sotto In Su','storybook illustration','surrealist art','surveillance footage','Tempera Painting','tilt shift','Trompe L’oeil','Ukiyo-e','unreal engine render','vector image','Veduta painting','visionary hypermaximalism','volumetric lighting','vray tracing']\nsites = ['500px','ArtStation','Behance','cgsociety','ConceptArtWorld','DeviantArt','Flickr','Getty Images','Pixiv','unsplash','zbrushcentral']\ngenre = ['anime','art deco','antique lithograph','concept','cyberpunk','dark fantasy','enlightenment','fantasy','fauvism','film noir','gothic','holography','linocut','massurrealism','medieval','monochrome','oil painting','pencil sketch','photoreal','post-impressionist','postmodern','psychedelic','renaissance','sci-fi','steampunk','clean vector','victorian','vintage','woodblock']\n\nprompt_ideas_map = {'artists': artists,'juice': juice, 'style': style, 'sites': sites, 'genre': genre}\n\ndef get_random_style():\n  styles = []\n  for k, v in prompt_ideas_map.items():\n    if k == 'artists':\n      # only 1 artist\n      if random.random() > 0.1:\n        artist = random.choice(v)\n        styles.append(f'by {artist}')\n    else:\n      count = random.randint(0, 3)\n      if count > 0:\n        styles.extend(random.sample(v, k=count))\n  return ', '.join(styles)\n\nget_random_style()\n\n'by Chris Rallis, dynamic composition, Gouache Painting, pencil sketch, Behance, cgsociety, gothic, monochrome, antique lithograph'\n\n\nNow let’s try our prompt generator: (change the seed to get different prompts)\n\nrandom.seed(42)\nprompt = \"a photo of a woman wearing a red dress, perfect face, \"\n\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\ncount = 4\nprompts = [prompt + get_random_style() for _ in range(count)]\nprint('\\n'.join(prompts))\n\nimage = text2image(\n    pipe,\n    prompt=prompts,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=[negative_prompt] * count,\n    num_images_per_prompt=1)\n\ndisplay(image)\n\na photo of a woman wearing a red dress, perfect face, by Alex Ross, insanely detailed and intricate, depth of field, surveillance footage\na photo of a woman wearing a red dress, perfect face, by Alfred Kubin, cgsociety\na photo of a woman wearing a red dress, perfect face, by Syd Mead, depth of field, professional photoshoot, elegant, Flickr, fauvism, cyberpunk\na photo of a woman wearing a red dress, perfect face, by Tony Sart, artstation, digital painting, Baroque painting, Impasto painting, Veduta painting, unsplash, ConceptArtWorld\n\n\n\n\n\n\n\n\nNow that’s much much better! We haven’t changed anything other than just the positive and negative prompts. If you have really bad images, the first thing you would like to change is the prompts.\nFor more ideas of prompts, https://lexica.art/ is a good place to find what other people are using."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "href": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Refine your image using Image2Image",
    "text": "Refine your image using Image2Image\nThe image to image pipeline allows you to start from an initial image rather than a random one.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n    model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)\n\n\n\n\nMost of the Image2Image pipeline parameters should look similar, except:\n\ninit_image this is the image you start from, i.e. the image you want to fix.\nstrength and steps: strength is a number between 0-1, meaning how much your output depends on your init image. For example a 0.6 strength for 100 steps means doing diffusion starting from 40% of the init image, and then do 60 steps.\n\n\nimport PIL\nimage = PIL.Image.open('image_to_fix.jpg')\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 200\nstrength=0.6\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\nimages = pipe_i2i(\n    init_image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    strength=strength,\n    num_inference_steps=num_inference_steps,\n    generator=generator,\n    guidance_scale=guidance_scale,\n    num_images_per_prompt=num_images_per_prompt\n).images\n\ndisplay(image)\ndisplay(image_grid(images))\n\n\n\n\n\n\n\n\n\n\nNote that for the image2image, I used way more steps - since I want to improve the quality.\nNow comparing the generated images to the initial one, there are some images that are better. The second one is already quite good, while the first one is good except for the eyes. How can we fix that? Let’s first save it:\n\nimages[0].save('image_fix_eye.jpg')"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "href": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Fixing your image using Inpainting",
    "text": "Fixing your image using Inpainting\nThe in-painting pipeline allows you to mask a region of the image, and re-generate only that region. Thus this is useful for adding small fixes to our images.\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, StableDiffusionInpaintPipeline, EulerAncestralDiscreteScheduler\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-inpainting\" #@param [\"stabilityai/stable-diffusion-2-inpainting\", \"runwayml/stable-diffusion-inpainting\"] {type:\"string\"}\n\nif model_id == \"runwayml/stable-diffusion-inpainting\":\n  inp_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n      model_id,\n      revision=\"fp16\",\n      torch_dtype=torch.float16,\n      use_auth_token=True\n  ).to(\"cuda\")\nelif model_id == \"stabilityai/stable-diffusion-2-inpainting\":\n  inp_pipe = DiffusionPipeline.from_pretrained(\n        model_id,\n        revision=\"fp16\",\n        torch_dtype=torch.float16,\n        # scheduler=scheduler # TODO currently setting scheduler here messes up the end result. A bug in Diffusers🧨\n      ).to(\"cuda\")\n  inp_pipe.scheduler = DPMSolverMultistepScheduler.from_config(inp_pipe.scheduler.config)\n  inp_pipe.enable_attention_slicing()\n  # inp_pipe.enable_xformers_memory_efficient_attention()\n\ninp_pipe.scheduler = EulerAncestralDiscreteScheduler(\n    num_train_timesteps=1000,\n    beta_end=0.012,\n    beta_start=0.00085,\n    beta_schedule=\"linear\",\n)\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\nTo use the in-painting pipeline, there is only one new paramter: - mask_image this is a black-white image where white pixels will be repainted, and black pixels will be preserved.\nFor the purpose of fixing our image, we just use the same prompts for generating this image. To ease the generation of a mask, we can use the gradio Web UI.\n\nimport gradio as gr\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n\ndef inpaint(input_image):\n  image, mask = input_image['image'], input_image['mask']\n  image = image.resize((512,512))\n  mask = mask.resize((512,512))\n\n  print('image:')\n  display(image)\n  print('mask:')\n  display(mask)\n\n  images = inp_pipe(\n      prompt=prompt,\n      negative_prompt=negative_prompt,\n      image=image,\n      mask_image=mask.convert('RGB'),\n      width=512,\n      height=512,\n      generator=generator,\n      guidance_scale=guidance_scale,\n      num_inference_steps=num_inference_steps,\n      num_images_per_prompt=num_images_per_prompt,\n  ).images\n\n  result = image_grid(images)\n  print('result:')\n  display(result)\n  return result\n\nwith gr.Blocks() as demo:\n  gr.Markdown('In painting demo')\n\n  with gr.Row():\n    with gr.Column():\n      input_image = gr.Image(label='Input', type = 'pil', tool='sketch', height=1024)\n      button = gr.Button('Inpaint')\n    with gr.Column():\n      output_image = gr.Image(label='Output', height=1024)\n\n  button.click(inpaint, inputs=input_image, outputs=output_image)\n\ndemo.launch(debug=True, share=True)\n\nBy Default the in painting works terrible when fixing a small region:\n\n\n\n\n\n\n\n\nBut If we apply a small trick: zoom in on the area, then fix a larger portion, it will do a much better job:\n\n\n\n\n\n\n\n\nThus, if we can programmatically “zoom in”, fix the region, then paste to the original image, we can do a much better job. This is left to the reader as an interesting excercise."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "href": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Upsampling your image",
    "text": "Upsampling your image\nYou can boost our image resolution even higher by using the upsampler. Note this takes a lot of GPU memory.\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\nupscale_pipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16).to(\"cuda\")\n\nWARNING:root:WARNING: /usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\nNeed to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop\n\n\n/usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\n\nimport PIL\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = PIL.Image.open('low_res.png').convert('RGB').resize((128,128))\n\nupscale_image = upscale_pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator\n).images[0]\n\ndisplay(image)\ndisplay(upscale_image)"
  },
  {
    "objectID": "posts/python/python-parallel-processing.html",
    "href": "posts/python/python-parallel-processing.html",
    "title": "Python Parellel Processing",
    "section": "",
    "text": "I came across this function called parallel in fastai, and it seems very interesting.\n\n\n\n\n\n\nA Simple Example\n\nfrom fastcore.all import parallel\n\n\nfrom nbdev.showdoc import doc\n\n\ndoc(parallel)\n\nparallel[source]parallel(f, items, *args, n_workers=8, total=None, progress=None, pause=0, **kwargs)\n\nApplies func in parallel to items, using n_workers\nShow in docs\n\n\nAs the documentation states, the parallel function can run any python function f with items using multiple workers, and collect the results.\nLet’s try a simple examples:\n\nimport math\nimport time\n\ndef f(x):\n  time.sleep(1)\n  return x * 2\n\nnumbers = list(range(10))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 s\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 32 ms, sys: 52 ms, total: 84 ms\nWall time: 2.08 s\n\n\nThe function f we have in this example is very simple: it sleeps for one second and then returns x*2. When executed in serial, it takes 10 seconds which is exactly what we expect. When using more workers(8 by default), it takes only 2 seconds.\n\n\nDig into the Implementation\nLet’s see how parallel is implemented:\n\nparallel??\n\n\nSignature:\nparallel(\n    f,\n    items,\n    *args,\n    n_workers=8,\n    total=None,\n    progress=None,\n    pause=0,\n    **kwargs,\n)\nSource:   \ndef parallel(f, items, *args, n_workers=defaults.cpus, total=None, progress=None, pause=0, **kwargs):\n    \"Applies `func` in parallel to `items`, using `n_workers`\"\n    if progress is None: progress = progress_bar is not None\n    with ProcessPoolExecutor(n_workers, pause=pause) as ex:\n        r = ex.map(f,items, *args, **kwargs)\n        if progress:\n            if total is None: total = len(items)\n            r = progress_bar(r, total=total, leave=False)\n        return L(r)\nFile:      /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:      function\n\n\n\n\n\n??ProcessPoolExecutor\n\n\nInit signature:\nProcessPoolExecutor(\n    max_workers=8,\n    on_exc=<built-in function print>,\n    pause=0,\n    mp_context=None,\n    initializer=None,\n    initargs=(),\n)\nSource:        \nclass ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n    \"Same as Python's ProcessPoolExecutor, except can pass `max_workers==0` for serial execution\"\n    def __init__(self, max_workers=defaults.cpus, on_exc=print, pause=0, **kwargs):\n        if max_workers is None: max_workers=defaults.cpus\n        self.not_parallel = max_workers==0\n        store_attr(self, 'on_exc,pause,max_workers')\n        if self.not_parallel: max_workers=1\n        super().__init__(max_workers, **kwargs)\n    def map(self, f, items, *args, **kwargs):\n        self.lock = Manager().Lock()\n        g = partial(f, *args, **kwargs)\n        if self.not_parallel: return map(g, items)\n        try: return super().map(partial(_call, self.lock, self.pause, self.max_workers, g), items)\n        except Exception as e: self.on_exc(e)\nFile:           /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we can see in the source code, under the hood, this is using the concurrent.futures.ProcessPoolExecutor class from Python.\nNote that this class is essentially different than Python Threads, which is subject to the Global Interpreter Lock.\nThe ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned.\n\n\nUse cases\nThis function can be quite useful for long running tasks and you want to take advantage of multi-core CPUs to speed up your processing. For example, if you want to download a lot of images from the internet, you may want to use this to parallize your download jobs.\nIf your function f is very fast, there can be suprising cases, here is an example:\n\nimport math\nimport time\n\ndef f(x):\n  return x * 2\n\nnumbers = list(range(10000))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 1.24 ms\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 3.96 s, sys: 940 ms, total: 4.9 s\nWall time: 12.4 s\n\n\nIn the above example, f is very fast and the overhead of creating a lot of tasks outweigh the advantage of multi-processing. So use this with caution, and always take profiles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chi’s Blog",
    "section": "",
    "text": "Stable Diffusion from Begginer to Master (2) Dreambooth\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion from Begginer to Master (1) Pipelines and Prompts\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPython Parellel Processing\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Chi is currently at Google, before which he has worked for Xiaomi, Amazon and PDD as a software engineer. His experiences include data and machine learning, especially in Ads and NLP."
  }
]