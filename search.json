[
  {
    "objectID": "posts/sd/stable-diffusion-2.html",
    "href": "posts/sd/stable-diffusion-2.html",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "",
    "text": "This is the second one of the Stable Diffusion tutorial series. In this tutorial, we will learn how to fine-tune the stable diffusion model on new images (aka Dreambooth)."
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#getting-images",
    "href": "posts/sd/stable-diffusion-2.html#getting-images",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Getting Images",
    "text": "Getting Images\nFor the purpose of this tutorial, let’s download some images from DuckDuckGo. Alternatively you can prepare your own images and put them inside a Google Drive Folder (referred to as images_src_dir in the next sections).\n\nimport pathlib\nfrom fastbook import search_images_ddg, download_images, verify_images, get_image_files\n\ndef download_images_for_keyword(keyword: str, download_dir: str, max_images):\n  dest = pathlib.Path(download_dir)\n  dest.mkdir(exist_ok=True)\n  results = search_images_ddg(keyword, max_images=max_images)\n  download_images(dest, urls=results)\n  downloaded_files = get_image_files(dest)\n  failed = verify_images(downloaded_files)\n  num_failed = len(failed)\n  if num_failed > 0:\n    print(f'Removing {num_failed} images')\n    failed.map(pathlib.Path.unlink)\n\nkeyword = 'Saito Asuka 2022' #@param {type: \"string\"}\ndownload_dir = '/content/drive/MyDrive/sd/images_download' #@param {type: \"string\"}\nmax_images = 60 #@param {type: \"slider\", min:20, max:100, step:1}\nforce_redownload = False #@param {type:\"boolean\"}\n\nif not pathlib.Path(download_dir).exists() or force_redownload:\n  download_images_for_keyword(keyword=keyword, download_dir=download_dir, max_images=max_images)\n\nRemoving 8 images"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#cropping-and-resizing",
    "href": "posts/sd/stable-diffusion-2.html#cropping-and-resizing",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Cropping and Resizing",
    "text": "Cropping and Resizing\nAlthough SD doesn’t put restrictions on image sizes (other than the width and height should be divisible by 8), we preform center crop and resize on all images to make them the same square shape, since the training batches need to be same dimensions.\nCropping might result in bad images, but no worries, we will clean them up in the next section.\n\nimport PIL\nimport pathlib\nfrom fastai.vision.widgets import ImagesCleaner\nfrom fastai.vision.all import PILImage\nfrom fastbook import get_image_files\n\nimages_src_dir = '/content/drive/MyDrive/sd/images_download' #@param {type: \"string\"}\nimages_train_dir = '/content/drive/MyDrive/sd/images_training' #@param {type: \"string\"}\nsize = 768 #@param {type: \"slider\", min:256, max:768, step:128}\nimages_dir = pathlib.Path(images_train_dir)\nimages_dir.mkdir(exist_ok=True)\n\nfor idx, image_file in enumerate(get_image_files(images_src_dir)):\n  im = PILImage.create(image_file)\n  im = im.crop_pad(min(im.size)).resize_max(max_h=size)\n  im.save(images_dir / image_file.name)\n\n\n!ls \"{images_src_dir}\" |wc -l\n!ls \"{images_train_dir}\" |wc -l\n\n50\n50"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#cleaning-up-images",
    "href": "posts/sd/stable-diffusion-2.html#cleaning-up-images",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Cleaning Up Images",
    "text": "Cleaning Up Images\nOne of the most important things of any ML applications, if not THE most important - is the data quality. To get best quality, let’s check our training images and remove the “bad” ones (especially the ones that doesn’t contain complete faces after cropping - we don’t want the final model to learn to generate half faces!)\nFastAI provides an ImagesCleaner class which is a very cool tool for removing images from the Jupyter notebook. Just select “Delete” for the images you want to delete, and then run the following cells to delete them.\n\nfns = get_image_files(images_train_dir)\nw = ImagesCleaner(max_n=100)\nw.set_fns(fns)\nw\n\n\n\n\n\n\n\nw.fns[w.delete()].map(pathlib.Path.unlink)\n\n(#30) [None,None,None,None,None,None,None,None,None,None...]\n\n\n\nfns = get_image_files(images_train_dir)\nw_clean = ImagesCleaner(max_n=100)\nw_clean.set_fns(fns)\nw_clean"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#using-the-diffusers-example-script",
    "href": "posts/sd/stable-diffusion-2.html#using-the-diffusers-example-script",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Using the diffusers example script",
    "text": "Using the diffusers example script\nFirst let’s train with the diffusers example script.\nThere are a few params worth noting:\n\nMODEL_NAME This is the id of your base model, e.g. SD1.5 or SD2.0. By default for training the SD2.0 with 768 resolution (“stabilityai/stable-diffusion-2”) you will need more than 16GB memory (about 22GB without xformers from my experiments).\nSTEPS The number of steps to train. Recommended value is ${num_examples} $.\nSAVE_STEPS The model will be exported every X steps, to avoid losing all progress when your GPU is recycled, or enable comparing different checkpoints to see which is best.\nOUTPUT_DIR This is where the trained model is exported.\nINSTANCE_DIR This points to the training images.\nINSTANCE_PROMPT This is the prompt for the training instances. In the diffusers example we are using a fixed prompt “a photo of xyz” for every instance image. This may not be optimal, and we’ll see how we can improve it later on.\n\nMake sure you selected GPU runtime (preferrably the best GPU you can choose) before continuing.\n\nMODEL_NAME=\"stabilityai/stable-diffusion-2\" #@param [\"runwayml/stable-diffusion-v1-5\", \"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\"]\nSTEPS=10000 #@param {type: \"integer\"}\nSAVE_STEPS=3000 #@param {type: \"integer\"}\nOUTPUT_DIR=\"/content/drive/MyDrive/sd/models/asuka\" #@param {type: \"string\"}\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\nINSTANCE_PROMPT=\"a photo of saitoasuka\" #@param {type: \"string\"}\nLEARNING_RATE=4e-6 #@param {type: \"number\"}\n\nif MODEL_NAME == \"stabilityai/stable-diffusion-2\":\n  resolution = 768\nelse:\n  resolution = 512\n\n!accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n  --train_text_encoder \\\n  --pretrained_model_name_or_path=\"{MODEL_NAME}\"  \\\n  --instance_data_dir=\"{INSTANCE_DIR}\" \\\n  --output_dir=\"{OUTPUT_DIR}\" \\\n  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n  --resolution={resolution} \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate={LEARNING_RATE} \\\n  --lr_scheduler=\"polynomial\" \\\n  --use_8bit_adam \\\n  --gradient_checkpointing \\\n  --lr_warmup_steps=0 \\\n  --save_steps={SAVE_STEPS} \\\n  --max_train_steps={STEPS}"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#using-blip-captions",
    "href": "posts/sd/stable-diffusion-2.html#using-blip-captions",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Using BLIP Captions",
    "text": "Using BLIP Captions\nIn the vanilla case, we used the same prompt for every image instance, which can be not ideal for the model - this is a training / testing mismatch. Because in training we have the same short prompt, but in validation we use very different prompts.\nBLIP is a model for generating text from images. We can use this model to automatically generate some informative descriptions and append to the prompts. This could help our model to generalize better.\n\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\n\nimport os\nimport sys\n\nif not os.path.exists('/content/BLIP'):\n  !rm -rf /content/BLIP\n  !git clone https://github.com/salesforce/BLIP.git\n  !pip install -Uqq timm fairscale pycocoevalcap\n\nif '/content/BLIP' not in sys.path:\n  sys.path.append('/content/BLIP')\n\nfrom models.blip import blip_decoder\nfrom torchvision import transforms\nimport PIL\nimport requests\nimport torch\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimage_size = 384\ntransform = transforms.Compose([\n  transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n  transforms.ToTensor(),\n  transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n])\n\nmodel_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\nmodel = blip_decoder(pretrained=model_url, med_config='/content/BLIP/configs/med_config.json', image_size=image_size, vit='large')\nmodel.eval()\nmodel = model.to(device)\n\ndef image2text(raw_image):\n  image = transform(raw_image).unsqueeze(0).to(device)\n  results = []\n  with torch.no_grad():\n    for _ in range(5):\n      caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n      results.extend(caption)\n  return results\n\nprint('generating blip captions')\nimage_captions = {}\nfor image_filename in os.listdir(INSTANCE_DIR):\n  image_filename_full = os.path.join(INSTANCE_DIR, image_filename)\n  image = PIL.Image.open(image_filename_full).convert('RGB')\n  text = image2text(image)\n  image_captions[image_filename_full] = text\n\nimport pickle\nwith open('image_captions.pickle', 'wb') as f:\n    pickle.dump(image_captions, f, protocol=pickle.HIGHEST_PROTOCOL)\nprint('finished generating blip captions')"
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#training-1",
    "href": "posts/sd/stable-diffusion-2.html#training-1",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "Training",
    "text": "Training\nWe are using the diffusers training scripts as a base, and make changes to the DreamboothDataset class so that it can now accept an additional parameter image_captions containing prompt mappings. For accelerate to run in Colab, we made some small changes, but those should be quite straightforward.\n\nPRETRAINED_MODEL = \"stabilityai/stable-diffusion-2-base\" #@param [\"runwayml/stable-diffusion-v1-5\", \"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\"] {allow-input: true, type: \"string\"}\nSTEPS=10000 #@param {type: \"integer\"}\nSAVE_STEPS=2000 #@param {type: \"integer\"}\nOUTPUT_DIR=\"/content/drive/MyDrive/sd/models/asuka_blip\" #@param {type: \"string\"}\nINSTANCE_DIR=\"/content/drive/MyDrive/sd/images_training\" #@param {type: \"string\"}\nINSTANCE_PROMPT=\"a photo of saitoasuka\" #@param {type: \"string\"}\nUSE_BLIP_CAPTIONS = True #@param {type:\"boolean\"}\nLEARNING_RATE=3e-6 #@param {type: \"number\"}\n\nfrom argparse import Namespace\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL\nimport random\nimport pickle\n\nargs = Namespace(\n    pretrained_model_name_or_path=PRETRAINED_MODEL, # Path to pretrained model or model identifier from huggingface.co/models.\n    revision=None,  # Revision of pretrained model identifier from huggingface.co/models.\n    tokenizer_name=None, # Pretrained tokenizer name or path if not the same as model_name\n    instance_data_dir=INSTANCE_DIR, # A folder containing the training data of instance images.\n    class_data_dir=None, # A folder containing the training data of class images.\n    instance_prompt=INSTANCE_PROMPT, # The prompt with identifier specifying the instance\n    class_prompt=None, # The prompt to specify images in the same class as provided instance images.\n    with_prior_preservation=False, # Flag to add prior preservation loss.\n    prior_loss_weight=1.0, # The weight of prior preservation loss.\n    num_class_images=100, # Minimal class images for prior preservation loss. If not have enough images, additional images will be sampled with class_prompt.\n    output_dir=OUTPUT_DIR, # The output directory where the model predictions and checkpoints will be written.\n    seed=None, # A seed for reproducible training.\n    resolution=768 if PRETRAINED_MODEL == \"stabilityai/stable-diffusion-2\" else 512, # The resolution for input images, all the images in the train/validation dataset will be resized to this resolution\n    center_crop=True, # Whether to center crop images before resizing to resolution\n    train_text_encoder=True, # Whether to train the text encoder\n    train_batch_size=1, # Batch size (per device) for the training dataloader.\n    sample_batch_size=4, # Batch size (per device) for sampling images.\n    num_train_epochs=1,\n    max_train_steps=STEPS, # Total number of training steps to perform.  If provided, overrides num_train_epochs.\n    save_steps=SAVE_STEPS, # Save checkpoint every X updates steps.\n    gradient_accumulation_steps=1, # Number of updates steps to accumulate before performing a backward/update pass.\n    gradient_checkpointing=True, # Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n    learning_rate=LEARNING_RATE, # Initial learning rate (after the potential warmup period) to use.\n    scale_lr=False, # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n    lr_scheduler=\"polynomial\", # The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n    lr_warmup_steps=0, # Number of steps for the warmup in the lr scheduler.\n    use_8bit_adam=True, # Whether or not to use 8-bit Adam from bitsandbytes\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_weight_decay=1e-2,\n    adam_epsilon=1e-8,\n    max_grad_norm=1.0,\n    push_to_hub=False,\n    hub_token=None,\n    hub_model_id=None,\n    logging_dir='logs',\n    mixed_precision=None, # [\"no\", \"fp16\", \"bf16\"]\n    local_rank=-1 # For distributed training: local_rank\n)\n\n\nif not os.path.exists('train_dreambooth.py'):\n  !cd /content/\n  !wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth.py -O train_dreambooth.py\n\nassert os.path.exists('train_dreambooth.py'), 'Unable to download train_dreambooth.py'\nimport train_dreambooth\ntrain_dreambooth.args = args\nfrom train_dreambooth import *\n\n\nif USE_BLIP_CAPTIONS:\n  with open('image_captions.pickle', 'rb') as f:\n    image_captions = pickle.load(f)\nelse:\n  image_captions = {}\nprint('Using image captions:', image_captions)\n\n\nclass DreamBoothDataset(Dataset):\n    \"\"\"\n    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n    It pre-processes the images and the tokenizes prompts.\n    \"\"\"\n\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        image_captions=None,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self.image_captions = image_captions or {}\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5]),\n            ]\n        )\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, index):\n        example = {}\n        image_path = self.instance_images_path[index % self.num_instance_images]\n        captions = self.image_captions.get(str(image_path))\n        if captions:\n          caption = random.choice(captions)\n          prompt = f'{self.instance_prompt}, {caption}'\n        else:\n          prompt = self.instance_prompt\n        instance_image = Image.open(image_path)\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            prompt,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                padding=\"do_not_pad\",\n                truncation=True,\n                max_length=self.tokenizer.model_max_length,\n            ).input_ids\n\n        return example\n\ndef display_example(tokenizer, ex):\n  image = ex['instance_images']\n  image_norm = ((image+1) / 2 * 255)\n  image_np = image_norm.numpy().astype(np.uint8).transpose((1,2,0))\n  prompt = tokenizer.decode(ex['instance_prompt_ids'])\n  raw_image = PIL.Image.fromarray(image_np)\n  print(prompt)\n  display(raw_image)\n\ndef display_training_examples():\n  tokenizer = AutoTokenizer.from_pretrained(\n      PRETRAINED_MODEL,\n      subfolder=\"tokenizer\",\n      revision=None,\n      use_fast=False,\n  )\n\n  train_dataset = DreamBoothDataset(\n      instance_data_root=INSTANCE_DIR,\n      instance_prompt=INSTANCE_PROMPT,\n      image_captions=image_captions,\n      class_data_root=None,\n      class_prompt=None,\n      tokenizer=tokenizer,\n      size=512,\n      center_crop=False,\n  )\n  print(\"Display a few training examples\")\n  for idx, ex in enumerate(train_dataset):\n    display_example(tokenizer, ex)\n    if idx >= 3:\n      break\n\ndef main(args):\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=\"tensorboard\",\n        logging_dir=logging_dir,\n    )\n\n    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n        raise ValueError(\n            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n        )\n\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    if args.with_prior_preservation:\n        class_images_dir = Path(args.class_data_dir)\n        if not class_images_dir.exists():\n            class_images_dir.mkdir(parents=True)\n        cur_class_images = len(list(class_images_dir.iterdir()))\n\n        if cur_class_images < args.num_class_images:\n            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n            pipeline = DiffusionPipeline.from_pretrained(\n                args.pretrained_model_name_or_path,\n                torch_dtype=torch_dtype,\n                safety_checker=None,\n                revision=args.revision,\n            )\n            pipeline.set_progress_bar_config(disable=True)\n\n            num_new_images = args.num_class_images - cur_class_images\n            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n\n            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\n            sample_dataloader = accelerator.prepare(sample_dataloader)\n            pipeline.to(accelerator.device)\n\n            for example in tqdm(\n                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n            ):\n                images = pipeline(example[\"prompt\"]).images\n\n                for i, image in enumerate(images):\n                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n                    image.save(image_filename)\n\n            del pipeline\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            repo = Repository(args.output_dir, clone_from=repo_name)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load the tokenizer\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.tokenizer_name,\n            revision=args.revision,\n            use_fast=False,\n        )\n    elif args.pretrained_model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.pretrained_model_name_or_path,\n            subfolder=\"tokenizer\",\n            revision=args.revision,\n            use_fast=False,\n        )\n\n    # import correct text encoder class\n    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n\n    # Load models and create wrapper for stable diffusion\n    text_encoder = text_encoder_cls.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=args.revision,\n    )\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"vae\",\n        revision=args.revision,\n    )\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"unet\",\n        revision=args.revision,\n    )\n\n    vae.requires_grad_(False)\n    if not args.train_text_encoder:\n        text_encoder.requires_grad_(False)\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        if args.train_text_encoder:\n            text_encoder.gradient_checkpointing_enable()\n\n    if args.scale_lr:\n        args.learning_rate = (\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n        )\n\n    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n    if args.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n            )\n\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    params_to_optimize = (\n        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n    )\n    optimizer = optimizer_class(\n        params_to_optimize,\n        lr=args.learning_rate,\n        betas=(args.adam_beta1, args.adam_beta2),\n        weight_decay=args.adam_weight_decay,\n        eps=args.adam_epsilon,\n    )\n\n    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n\n    print('Image captions:', image_captions)\n    train_dataset = DreamBoothDataset(\n        image_captions=image_captions,\n        instance_data_root=args.instance_data_dir,\n        instance_prompt=args.instance_prompt,\n        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n        class_prompt=args.class_prompt,\n        tokenizer=tokenizer,\n        size=args.resolution,\n        center_crop=args.center_crop,\n    )\n\n    def collate_fn(examples):\n        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n        pixel_values = [example[\"instance_images\"] for example in examples]\n\n        # Concat class and instance examples for prior preservation.\n        # We do this to avoid doing two forward passes.\n        if args.with_prior_preservation:\n            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n            pixel_values += [example[\"class_images\"] for example in examples]\n\n        pixel_values = torch.stack(pixel_values)\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\n        input_ids = tokenizer.pad(\n            {\"input_ids\": input_ids},\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        batch = {\n            \"input_ids\": input_ids,\n            \"pixel_values\": pixel_values,\n        }\n        return batch\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=1\n    )\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n\n    if args.train_text_encoder:\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n        )\n    else:\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, optimizer, train_dataloader, lr_scheduler\n        )\n\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    # Move text_encode and vae to gpu.\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n    # as these models are only used for inference, keeping weights in full precision is not required.\n    vae.to(accelerator.device, dtype=weight_dtype)\n    if not args.train_text_encoder:\n        text_encoder.to(accelerator.device, dtype=weight_dtype)\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if accelerator.is_main_process:\n        accelerator.init_trackers(\"dreambooth\", config=vars(args))\n\n    # Train!\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Steps\")\n    global_step = 0\n\n    for epoch in range(args.num_train_epochs):\n        unet.train()\n        if args.train_text_encoder:\n            text_encoder.train()\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                # Convert images to latent space\n                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n                latents = latents * 0.18215\n\n                # Sample noise that we'll add to the latents\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                # Sample a random timestep for each image\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n                timesteps = timesteps.long()\n\n                # Add noise to the latents according to the noise magnitude at each timestep\n                # (this is the forward diffusion process)\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n                # Get the text embedding for conditioning\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n                # Predict the noise residual\n                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n                # Get the target for loss depending on the prediction type\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                if args.with_prior_preservation:\n                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n                    target, target_prior = torch.chunk(target, 2, dim=0)\n\n                    # Compute instance loss\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n\n                    # Compute prior loss\n                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n\n                    # Add the prior loss to the instance loss.\n                    loss = loss + args.prior_loss_weight * prior_loss\n                else:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    params_to_clip = (\n                        itertools.chain(unet.parameters(), text_encoder.parameters())\n                        if args.train_text_encoder\n                        else unet.parameters()\n                    )\n                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n\n                if global_step % args.save_steps == 0:\n                    if accelerator.is_main_process:\n                        pipeline = DiffusionPipeline.from_pretrained(\n                            args.pretrained_model_name_or_path,\n                            unet=accelerator.unwrap_model(unet),\n                            text_encoder=accelerator.unwrap_model(text_encoder),\n                            revision=args.revision,\n                        )\n                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                        pipeline.save_pretrained(save_path)\n\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        accelerator.wait_for_everyone()\n\n    # Create the pipeline using using the trained modules and save it.\n    if accelerator.is_main_process:\n        pipeline = DiffusionPipeline.from_pretrained(\n            args.pretrained_model_name_or_path,\n            unet=accelerator.unwrap_model(unet),\n            text_encoder=accelerator.unwrap_model(text_encoder),\n            revision=args.revision,\n        )\n        pipeline.save_pretrained(args.output_dir)\n\n        if args.push_to_hub:\n            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n\n    accelerator.end_training()\n\n# display_training_examples()\nimport accelerate\naccelerate.notebook_launcher(main, args=(args,))\n\nHere are the results using same settings:\n\n\n\n\n\n\n\n\n\n\nYou can see from the above result that the model is now respecting the “wearing a hat” prompt.\nHowever the model didn’t get the “baseball hat” right. After tuning some other parameters, I’ve got some better results below:\n\n\n\n\n\nCan you guess what I changed? This is left as an excersise."
  },
  {
    "objectID": "posts/sd/stable-diffusion-2.html#optional-xformers",
    "href": "posts/sd/stable-diffusion-2.html#optional-xformers",
    "title": "Stable Diffusion from Begginer to Master (2) Dreambooth",
    "section": "[Optional] xformers",
    "text": "[Optional] xformers\nxformers allows memory efficient attention to save GPU memory. If not working, try following command: (takes a lot of time!)\n\n!pip install -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers"
  },
  {
    "objectID": "posts/sd/stable-diffusion-4.html",
    "href": "posts/sd/stable-diffusion-4.html",
    "title": "Stable Diffusion from Begginer to Master (4) Text Encoder and Tokenizer",
    "section": "",
    "text": "In this tutorial we’ll take a deeper look into the text processing components of stable diffusion - the TextEncoder and the Tokenizer.\n\nSetup\n\n!pip install -Uqq diffusers transformers ftfy accelerate bitsandbytes\n#!pip install -Uqq triton xformers\n\n\n\nPipeline\nWe use the same stable diffusion pipeline from last tutorial:\n\nimport pprint\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', revision=\"fp16\", torch_dtype=torch.float16).to(device)\n\n\n\n\nNow we can access the text encoder and tokenizer by:\n\npipe.tokenizer\n\nPreTrainedTokenizer(name_or_path='/root/.cache/huggingface/diffusers/models--stabilityai--stable-diffusion-2-base/snapshots/1cb61502fc8b634cdb04e7cd69e06051a728bedf/tokenizer', vocab_size=49408, model_max_len=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '!'})\n\n\n\npipe.text_encoder\n\n\n\nTokenizer\nThe Tokenizer does two things:\n\nBreaks down a long text into “tokens” (the tokenize method).\nConverts tokens into a list of integer ids with value range 0 ~ vocabsize-1 , which are indices into an embedding matrix(the convert_tokens_to_ids method). This is essentially just a dictionary lookup.\n\n\ntokens = pipe.tokenizer.tokenize(\"a highres photo of a woman wearing a red dress\")\nprint(tokens)\n\n['a</w>', 'high', 'res</w>', 'photo</w>', 'of</w>', 'a</w>', 'woman</w>', 'wearing</w>', 'a</w>', 'red</w>', 'dress</w>']\n\n\nTokens are not necessarily one-to-one with words - you can see from the above example that highres is broken into two words - high and res</w>. The symbol </w> inidicates the end of a word, so for example of and of</w> are two different tokens, the former meaning the of is in the middle of some other word. This clever and powerful idea allows us to process words that are not seen in the training data by breaking them into “subwords” that exist in the vocabulary.\nNow we can convert the tokens to integer ids:\n\npipe.tokenizer.convert_tokens_to_ids(tokens)\n\n[320, 1487, 934, 1125, 539, 320, 2308, 3309, 320, 736, 2595]\n\n\nTo allow batch processing, we always pad the ids to a fixed max length (77 in the stable diffusion case). If there are more than this number of tokens, the text gets truncated.\nTo know where the paddings start and end, we also construct a mask that marks the text as 1s and paddings as 0s.\nTo do all these processing in one go, we can use the tokenizer() method:\n\ntext_inputs = pipe.tokenizer(\n    [\"a highres photo of a woman wearing a red dress\",\n     ' '.join([\"a\"] + [\"very\"]*100 + [\"text\"])],\n    padding=\"max_length\",\n    max_length=pipe.tokenizer.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\")\npprint.pprint(text_inputs)\n\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1]]),\n 'input_ids': tensor([[49406,   320,  1487,   934,  1125,   539,   320,  2308,  3309,   320,\n           736,  2595, 49407,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0],\n        [49406,   320,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n          1070,  1070,  1070,  1070,  1070,  1070, 49407]])}\n\n\nComparing this result with the previous one, seems there are two addtional tokens 49406 and 49407 in the beginning and end of the integer ids list.\nIf we want to debug the input ids, we can also use the tokenizer to decode them:\n\npipe.tokenizer.decode(torch.tensor([49406,   320,  1487,   934,  1125,   539,   320,  2308,  3309,   320, 736,  2595, 49407]))\n\n'<|startoftext|>a highres photo of a woman wearing a red dress <|endoftext|>'\n\n\nThe tokenizer conventionally adds <|startoftext|> and <|endoftext|> tokens to the sentence. These tokens help the model to learn the beginning and end of the sentence.\nLet’s check how many tokens there are in the vocabulary:\n\npipe.tokenizer.vocab_size\n\n49408\n\n\nWhat are the special tokens?\n\npipe.tokenizer.special_tokens_map\n\n{'bos_token': '<|startoftext|>',\n 'eos_token': '<|endoftext|>',\n 'pad_token': '!',\n 'unk_token': '<|endoftext|>'}\n\n\nHow does integer correspond to tokens?\n\npipe.tokenizer.decoder[320], pipe.tokenizer.decoder[1125], pipe.tokenizer.decoder[49406], pipe.tokenizer.decoder[49407]\n\n('a</w>', 'photo</w>', '<|startoftext|>', '<|endoftext|>')\n\n\nHow is an unknown word handled?\nI haven’t found a way to get a out-of-vocabulary token, since the vocabulary contains all possible bytes, which serves as a fall back.\n\npipe.tokenizer.tokenize('你好, abcd, asdfhjklmn')\n\n['ä½',\n 'ł',\n 'å¥',\n '½</w>',\n ',</w>',\n 'ab',\n 'cd</w>',\n ',</w>',\n 'asdf',\n 'h',\n 'j',\n 'kl',\n 'mn</w>']\n\n\nTo see the mapping between integer indices and tokens, use the get_vocab method.\n\npipe.tokenizer('abcd, asdfhjklmn')\n\n{'input_ids': [49406, 596, 4480, 267, 36857, 71, 73, 8498, 4057, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\nidx2token = {idx: token for token, idx in pipe.tokenizer.get_vocab().items() }\n[idx2token[i] for i in pipe.tokenizer('abcd, asdfhjklmn')['input_ids']]\n\n['<|startoftext|>',\n 'ab',\n 'cd</w>',\n ',</w>',\n 'asdf',\n 'h',\n 'j',\n 'kl',\n 'mn</w>',\n '<|endoftext|>']\n\n\nHow are punctuations preprocessed?\nNothing in particular - as seen in the above example, punctuations like commas usually have their own token (,</w>).\nTo know more about tokenizers, check out the huggingface tutorial.\n\n\nText Encoder\nThe text encoder is a CLIPTextModel.\nIt is a transfomer model that takes token ids from tokenizer as input and get an embedding of text.\n\ninputs = pipe.tokenizer([\"a photo of a cat\", \"a photo of a woman wearing a red dress\"],\n                        padding=True, return_tensors=\"pt\").to(\"cuda\")\npprint.pprint(inputs)\noutputs = pipe.text_encoder(**inputs)\npprint.pprint(outputs)\n\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n 'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407,     0,     0,     0,\n             0],\n        [49406,   320,  1125,   539,   320,  2308,  3309,   320,   736,  2595,\n         49407]], device='cuda:0')}\n{'last_hidden_state': tensor([[[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n         ...,\n         [-0.0199, -0.2195, -0.0608,  ...,  0.1279,  0.1672, -0.1105],\n         [-0.0690, -0.2585, -0.0515,  ...,  0.1525,  0.1367, -0.1448],\n         [-0.0992, -0.2791, -0.0477,  ...,  0.1680,  0.1204, -0.1660]],\n\n        [[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n         ...,\n         [ 0.2007,  0.2732, -0.4333,  ...,  1.0098, -1.6348,  0.8604],\n         [ 0.2339,  2.2188, -0.5488,  ...,  0.0466, -2.1484,  0.1888],\n         [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>),\n 'pooler_output': tensor([[ 0.3347, -0.0175,  1.0537,  ...,  0.6938, -0.4158, -1.3076],\n        [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)}\n\n\nSo by default the input is padded to the max token length of the batch of text.\nDoes padding matter when calling text encoder? Let’s see what happens if the inputs are padded to 77 tokens.\n\ninputs1 = pipe.tokenizer([\"a photo of a cat\", \"a photo of a woman wearing a red dress\"],\n                         padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\").to(\"cuda\")\npprint.pprint(inputs1)\noutputs1 = pipe.text_encoder(**inputs)\npprint.pprint(outputs1)\n\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]], device='cuda:0'),\n 'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0],\n        [49406,   320,  1125,   539,   320,  2308,  3309,   320,   736,  2595,\n         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')}\n{'last_hidden_state': tensor([[[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n         ...,\n         [-0.0199, -0.2195, -0.0608,  ...,  0.1279,  0.1672, -0.1105],\n         [-0.0690, -0.2585, -0.0515,  ...,  0.1525,  0.1367, -0.1448],\n         [-0.0992, -0.2791, -0.0477,  ...,  0.1680,  0.1204, -0.1660]],\n\n        [[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n         ...,\n         [ 0.2007,  0.2732, -0.4333,  ...,  1.0098, -1.6348,  0.8604],\n         [ 0.2339,  2.2188, -0.5488,  ...,  0.0466, -2.1484,  0.1888],\n         [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>),\n 'pooler_output': tensor([[ 0.3347, -0.0175,  1.0537,  ...,  0.6938, -0.4158, -1.3076],\n        [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)}\n\n\nWe can check that padding does not affect encoder result at all:\n\ntorch.equal(outputs.last_hidden_state, outputs1.last_hidden_state)\n\nTrue\n\n\nNow we can take a closer look at the text encoding model:\n\npipe.text_encoder.text_model\n\nCLIPTextTransformer(\n  (embeddings): CLIPTextEmbeddings(\n    (token_embedding): Embedding(49408, 1024)\n    (position_embedding): Embedding(77, 1024)\n  )\n  (encoder): CLIPEncoder(\n    (layers): ModuleList(\n      (0): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (12): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (13): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (14): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (15): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (16): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (17): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (18): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (19): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (20): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (21): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (22): CLIPEncoderLayer(\n        (self_attn): CLIPAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): CLIPMLP(\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)\n\n\nThe text model is a CLIPTextTransformer, described in detail in the CLIP paper.\n\nfrom transformers.models.clip.modeling_clip import CLIPTextTransformer\n\n\nCLIPTextTransformer??"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#setup",
    "href": "posts/sd/stable-diffusion-1.html#setup",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Setup",
    "text": "Setup\nTo begin our journey, first we need to install some packages.\n\ndiffusers This is the package that we are mainly using for generating images.\ntransformers This is the package to encode text into embeddings.\nAnd a few other supporting packages to work with the above packages.\n\n\n!pip install -Uqq diffusers transformers ftfy scipy accelerate gradio xformers triton==2.0.0.dev20221120\nimport pathlib\nimport huggingface_hub\nif not pathlib.Path('/root/.huggingface/token').exists():\n  huggingface_hub.notebook_login()\n\nToken is valid.\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /root/.huggingface/token\nLogin successful"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#utility-functions",
    "href": "posts/sd/stable-diffusion-1.html#utility-functions",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Utility Functions",
    "text": "Utility Functions\n\nimport PIL\nimport math\n\ndef image_grid(imgs, rows=None, cols=None) -> PIL.Image.Image:\n  n_images = len(imgs)\n  if not rows and not cols:\n    cols = math.ceil(math.sqrt(n_images))\n  if not rows:\n    rows = math.ceil(n_images / cols)\n  if not cols:\n    cols = math.ceil(n_images / rows)\n\n  w, h = imgs[0].size\n  grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n\n  for i, img in enumerate(imgs):\n      grid.paste(img, box=(i%cols*w, i//cols*h))\n  return grid"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#initializing-the-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Initializing the Pipeline",
    "text": "Initializing the Pipeline\nThere are two important params to intialize a Pipeline. a model_id and a revision.\n\nThe model_id can be either a huggingface model id or some path in your file system. To find which ones to use, go to huggingface and look for model id at the top. Later when we train our own models we will pass in the path to our trained model.\nThe revision can be set fp16 to save GPU memory by using 16 bit numbers.\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "href": "posts/sd/stable-diffusion-1.html#generating-image-using-the-text-to-image-pipeline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Generating image using the text to image pipeline",
    "text": "Generating image using the text to image pipeline\nTo generate an image from text, we just call the pipe() method. There are a few arguments to the method:\n\nprompt A text describing the thing you want the image to be.\nnegative_prompt A text describing the features that you don’t want the image to have.\ngenerator This is a random number generator. By default the generator is None, and the output is random for same prompts. A fixed seed allows repeatable experiment (same input-> same output).\nwidth and height the dimension of output image.\nguidance_scale A scale determining the extent of how your image accurately matches your prompt. In practice I find it not very useful to tune this, just use a 7.5 you will be fine.\nnum_inference_steps How many steps to run the diffusion algorithm(will explain later). The more steps, the better the image quality, and the more time it takes to generate an image.\n\n\ndef text2image(pipe,\n               prompt: str,\n               seed: int,\n               return_grid=False,\n               grid_size=None,\n               **kwargs):\n  generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n  with torch.autocast(\"cuda\"):\n    images = pipe(prompt,\n                  generator=generator,\n                  **kwargs).images\n\n  if len(images) == 1:\n    return images[0]\n  elif return_grid:\n    return image_grid(images)\n  else:\n    return images"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#baseline",
    "href": "posts/sd/stable-diffusion-1.html#baseline",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Baseline",
    "text": "Baseline\nFirst let’s try using a prompt with all default parameters:\n\nprompt = \"a photo of a woman wearing a red dress\"\nnegative_prompt = \"\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s terrible! If you get completely unusable images like the above, the first thing to change is the prompts."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "href": "posts/sd/stable-diffusion-1.html#add-prompts-and-negative-prompts",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Add Prompts and Negative Prompts",
    "text": "Add Prompts and Negative Prompts\nLet’s add a few style keywords to the prompt, and then some negative keywords:\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\n\nimage = text2image(\n    pipe,\n    prompt=prompt,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=num_images_per_prompt)\n\ndisplay(image)\n\n\n\n\n\n\n\nIt’s far from perfect, but much better than our first try. Generally speaking stable diffusion version 2 works much worse than version 1.5 by default, and it needs much more effort in tuning the prompts.\nThe negative prompt usually works well for many different images, but for the positive prompts there can be a lot of possiblities.\nNow let’s build a systematic way of finding prompt additions:"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "href": "posts/sd/stable-diffusion-1.html#a-random-prompt-generator",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "A random prompt generator",
    "text": "A random prompt generator\n\nimport random\n\nartists = ['Aaron Douglas','Agnes Lawrence Pelton','Akihiko Yoshida','Albert Bierstadt','Albert Bierstadt','Alberto Giacometti',\n           'Alberto Vargas','Albrecht Dürer','Aleksi Briclot','Alex Grey','Alex Horley-Orlandelli','Alex Katz','Alex Ross',\n           'Alex Toth','Alexander Jansson','Alfons Maria Mucha','Alfred Kubin','Alphonse Mucha','Anato Finnstark','Anders Zorn',\n           'André Masson','Andreas Rocha','Andrew Wyeth','Anish Kapoor','Anna Dittmann','Anna Mary Robertson Moses','Anni Albers',\n           'Ansel Adams','Anthony van Dyck','Anton Otto Fischer','Antonio Mancini','April Gornik','Arnold Böcklin','Art Spiegelman',\n           'Artemisia Gentileschi','Artgerm','Arthur Garfield Dove','Arthur Rackham','Asher Brown Durand','Aubrey Beardsley',\n           'Austin Briggs','Ayami Kojima','Bastien Lecouffe-Deharme','Bayard Wu','Beatrix Potter','Beeple','Beksinski',\n           'Bill Sienkiewicz','Bill Ward','Bill Watterson','Bob Eggleton','Boris Vallejo','Brian Bolland','Brian Froud',\n           'Bruce Pennington','Bunny Yeager','Camille Corot','Camille Pissarro','Canaletto','Caravaggio','Caspar David Friedrich',\n           'Cedric Peyravernay','Charles Addams','Charles Dana Gibson','Chesley Bonestell','Chris Foss','Chris Moore',\n           'Chris Rallis','Chriss Foss','Cindy Sherman','Clara Peeters','Claude Monet','Clyde Caldwell','Coles Phillips',\n           'Cornelis Bisschop','Coby Whitmore','Craig Mullins','Cynthia Sheppard','Dale Chihuly','Damien Hirst','Dan Mumford',\n           'Daniel Merriam','Darek Zabrocki','Dave Dorman','Dave Gibbons','Dave McKean','David Firth','Dean Cornwell','Dean Ellis',\n           'Diane Dillon','Disney','Don Maitz','Donato Giancola','Dorothea Tanning','Dreamworks','Dr. Seuss','Earl Norem',\n           'Earle Bergey','Earnst Haeckel','Ed Emshwiller','Edgar Degas','Edmund Dulac','Edmund Leighton','Édouard Manet',\n           'Edvard Munch','Edward Burne-Jones','Edward Gorey','Edward Hopper','Edward Lear','Edwin Austin Abbey','Edwin Deakin',\n           'Egon Schiele','El Greco','Elizabeth Shippen Green','Emmanuel Shiu','Emory Douglas','Esao Andrews','Eugène Delacroix',\n           'Evelyn De Morgan','E.H. Shepard','F. Scott Hess','Fairfield Porter','Federico Pelat','Filippino Lippi','Fitz Henry Lane',\n           'Francis Bacon','Francisco Goya','Frank Frazetta','Frank Xavier Leyendecker','Franklin Booth','Franz Sedlacek',\n           'Frederick Edwin Church','Frederick McCubbin','Gaston Bussière','Gediminas Pranckevicius','Geof Darrow',\n           'George B. Bridgman','George Cruikshank','George Inness','George Luks',\"Georgia O'Keeffe\",'Gerald Brom','Giacomo Balla',\n           'Gil Elvgren','Gillis Rombouts','Giorgio de Chirico','Giorgione','Giovanni Battista Piranesi','Greg Hildebrandt',\n           'Greg Rutkowski','Greg Staples','Gregory Manchess','Guido Borelli da Caluso','Gustaf Tenggren','Gustav Klimt',\n           'Gustave Doré','Gustave Moreau','Gwen John','Hannah Höch','Hans Baldung','Hans Bellmer','Harrison Fisher','Harvey Dunn',\n           'Harvey Kurtzman','Henri de Toulouse-Lautrec','Henri Matisse','Henri Rousseau','Henry Ossawa Tanner','Henry Raleigh',\n           'Hethe Srodawa','Hieronymus Bosch','Hiromu Arakawa','Hokusai','Howard Chandler Christy','Howard Pyle','Hubert Robert',\n           'Hugh Ferriss','Hyun Lee','H.R. Giger','Igor Kieryluk','Igor Morski','Igor Wolkski','Ilya Kuvshinov','Ilya Repin',\n           'Inyi Han','Isaac Levitan','Ivan Aivazovsky','Ivan Albright','Ivan Bilibin','Ivan Shishkin','Jacek Yerka','Jack Kirby',\n           'Jackson Pollock','Jakub Rozalski','James C. Christensen','James Gillray','James Gurney','James Jean','James Paick',\n           'Jamie Hewlett','Jan van Eyck','Janet Fish','Jasper Johns','J.C. Leyendecker','Jean Delville','Jean Giraud',\n           'Jean Metzinger','Jean-Honoré Fragonard','Jean-Michel Basquiat','Jeff Easley','Jeff Koons','Jeffrey Smith',\n           'Jerome Lacoste','Jerry Pinkney','Jesper Ejsing','Jessie Willcox Smith','Jim Burns','Jim Steranko','Joaquín Sorolla',\n           'Joe Jusko','Johannes Vermeer','Johfra Bosschart','John Atkinson Grimshaw','John Bauer','John Berkey','John Constable',\n           'John Frederick Kensett','John French Sloan','John Harris','John Howe','John James Audubon','John Martin',\n           'John Philip Falter','John Romita Jr','Jon Foster','Jon Whitcomb','Joseph Cornell','Juan Gris','Junji Ito',\n           'J.M.W. Turner','Kadir Nelson','Kandinsky','Karol Bak','Kate Greenaway','Kawanabe Kyōsai','Kay Nielsen',\n           'Keith Haring','Kelly Freas','Kelly Mckernan','Kim Jung Gi','Kinuko Craft','Konstantin Vasilyev',\n           'Konstantinas Ciurlionis','Krenz Cushart','Lale Westvind','Larry Elmore','Laura Muntz Lyall','Laurel Burch',\n           'Laurie Lipton','Lawren Harris','Lee Madgwick','Leo and Diane Dillon','Leonora Carrington','Liam Wong','Lise Deharme',\n           'Lois van Baarle','Louis Glackens','Louis Janmot','Louise Bourgeois','Lucian Freud','Luis Royo','Lynda Benglis',\n           'Lyubov Popova','Maciej Kuciara','Makoto Shinkai','Malevich','Marc Simonetti','Margaret Macdonald Mackintosh',\n           'Maria Sibylla Merian','Marianne North','Mario Sanchez Nevado','Mark Ryden','Martin Johnson Heade','Mary Cassatt',\n           'Mati Klarwein','Maxfield Parrish','Mead Schaeffer','Michael Hussar','Michael Parkes','Michael Whelan',\n           'Mikalojus Konstantinas Čiurlionis','Mike Mignola','Milton Caniff','Milton Glaser','Moebius','Mondrian','M.C. Escher',\n           'Noah Bradley','Noriyoshi Ohrai','Norman Rockwell','N.C. Wyeth','Odd Nerdrum','Odilon Redon','Ohara Koson',\n           'Paul Cézanne','Paul Delvaux','Paul Gauguin','Paul Klee','Paul Lehr','Peter Elson','Peter Gric','Peter Helck',\n           'Peter Max','Peter Mohrbacher','Peter Paul Rubens','Pierre Bonnard','Pierre-Auguste Renoir','Pieter Bruegel the Elder',\n           'Pieter Claesz','Pixar','P.A. Works','Rafal Olbinski','Ralph Horsley','Ralph McQuarrie','Randolph Caldecott',\n           'Raphael Lacoste','Ray Caesar','Raymond Swanland','Rebecca Guay','Rembrandt','Rembrandt van Rijn','Rene Magritte',\n           'RHADS','Richard Dadd','Richter','Rob Gonsalves','Robert Delaunay','Robert McCall','Robert McGinnis',\n           'Robert Rauschenberg','Roberto da Matta','Rockwell Kent','Rodney Matthews','Roger Ballen','Roger Dean','Ron Walotsky',\n           'Rossdraws','Ross Tran','Roz Chast','Salvador Dalí','Sam Spratt','Sandro Botticelli','Saul Steinberg','Saul Tepper',\n           'Seb McKinnon','Simon Bisley','Simon Stalenhag','Sir John Tenniel','Slawomir Maniak','Sonia Delaunay','sparth',\n           'Stephan Martiniere','Stevan Dohanos','Steve Dillon','Steven DaLuz','Studio Ghibli','Syd Mead','Sylvain Sarrailh',\n           'Takashi Murakami','Takato Yamamoto','Takeshi Obata','Tamara Lempicka','Taro Okamoto','Ted DeGrazia','Ted Nasmith',\n           'Terry Oakes','Terry Redlin','Thomas Cole','Thomas Kinkade','Thomas Nast','Thornton Oakley','Brothers Hildebrandt',\n           'Tim White','Titian','Tom Lovell','Tom Thomson','Tomek Setowski','Tomer Hanuka','Tomi Ungerer','Tomokazu Matsuyama',\n           'Tony Sart','Tsutomu Nihei','Tyler Edlin','Utagawa Kuniyoshi','Victo Ngai','Vincent Di Fate','Vladimir Kush',\n           'Wally Wood','Walter Beach Humphrey','Walter Crane','Warwick Goble','Wassily Kandinsky','Wayne Barlowe','Wendy Froud',\n           'Wifredo Lam','Will Eisner','William Hogarth','William Michael Harnett','William Steig','William Stout',\n           'William-Adolphe Bouguereau','Winslow Homer','Winsor McCay','WLOP','Yayoi Kusama','Yoshitaka Amano','Yue Minjun',\n           'Yves Tanguy','Zdzisław Beksiński']\n\njuice = ['dynamic composition','cinematic lighting','intricate','studio quality','highly detailed',\n          'digital painting', 'artstation', 'matte', 'sharp focus','hyper detailed', 'super sharp',\n          'crisp', 'smooth', 'smooth gradients', 'depth of field','insanely detailed and intricate',\n          'hypermaximalist', 'elegant', 'ornate', 'hyper realistic', 'super detailed', 'cinematic light',\n          'ray tracing', 'volumetric lighting', 'octane render','cinematic lighting', 'highly detailed',\n          'sharp focus', 'professional photoshoot', '8k', 'DOF','dramatically lit', '1ms shutter speed',\n          'back lighting', 'F 2.8 lens']\n\nstyle = ['2d game art','3D VR painting','8k resolution','1950s pulp sci-fi cover','anime','artistic photograph','Baroque painting','Byzantine mosaic','Chiaroscuro painting','depth of field','digital painting','dutch golden age','filmed in IMAX','fine art','flat shading','Flemish Baroque','Fresco painting','Gouache Painting','graffiti','Grisaille painting','highly detailed','hyperrealism','Impasto painting','low-poly','Luminism painting','Marvel Comics','matte painting','mixed media','oil painting','Panorama','parallax','pastel painting','pencil sketch','Perspective painting','Playstation 5 screenshot','pop art','raytracing','rendered in cinema4d','rendered in maya','rendered in zbrush','schematic','sculpture','Sfumato painting','shot on 70mm','Sotto In Su','storybook illustration','surrealist art','surveillance footage','Tempera Painting','tilt shift','Trompe L’oeil','Ukiyo-e','unreal engine render','vector image','Veduta painting','visionary hypermaximalism','volumetric lighting','vray tracing']\nsites = ['500px','ArtStation','Behance','cgsociety','ConceptArtWorld','DeviantArt','Flickr','Getty Images','Pixiv','unsplash','zbrushcentral']\ngenre = ['anime','art deco','antique lithograph','concept','cyberpunk','dark fantasy','enlightenment','fantasy','fauvism','film noir','gothic','holography','linocut','massurrealism','medieval','monochrome','oil painting','pencil sketch','photoreal','post-impressionist','postmodern','psychedelic','renaissance','sci-fi','steampunk','clean vector','victorian','vintage','woodblock']\n\nprompt_ideas_map = {'artists': artists,'juice': juice, 'style': style, 'sites': sites, 'genre': genre}\n\ndef get_random_style():\n  styles = []\n  for k, v in prompt_ideas_map.items():\n    if k == 'artists':\n      # only 1 artist\n      if random.random() > 0.1:\n        artist = random.choice(v)\n        styles.append(f'by {artist}')\n    else:\n      count = random.randint(0, 3)\n      if count > 0:\n        styles.extend(random.sample(v, k=count))\n  return ', '.join(styles)\n\nget_random_style()\n\n'by Chris Rallis, dynamic composition, Gouache Painting, pencil sketch, Behance, cgsociety, gothic, monochrome, antique lithograph'\n\n\nNow let’s try our prompt generator: (change the seed to get different prompts)\n\nrandom.seed(42)\nprompt = \"a photo of a woman wearing a red dress, perfect face, \"\n\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nwidth = height = 512\nnum_inference_steps = 30\nguidance_scale = 7.5\ncount = 4\nprompts = [prompt + get_random_style() for _ in range(count)]\nprint('\\n'.join(prompts))\n\nimage = text2image(\n    pipe,\n    prompt=prompts,\n    seed=seed,\n    return_grid=True,\n    width=width,\n    height=height,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    negative_prompt=[negative_prompt] * count,\n    num_images_per_prompt=1)\n\ndisplay(image)\n\na photo of a woman wearing a red dress, perfect face, by Alex Ross, insanely detailed and intricate, depth of field, surveillance footage\na photo of a woman wearing a red dress, perfect face, by Alfred Kubin, cgsociety\na photo of a woman wearing a red dress, perfect face, by Syd Mead, depth of field, professional photoshoot, elegant, Flickr, fauvism, cyberpunk\na photo of a woman wearing a red dress, perfect face, by Tony Sart, artstation, digital painting, Baroque painting, Impasto painting, Veduta painting, unsplash, ConceptArtWorld\n\n\n\n\n\n\n\n\nNow that’s much much better! We haven’t changed anything other than just the positive and negative prompts. If you have really bad images, the first thing you would like to change is the prompts.\nFor more ideas of prompts, https://lexica.art/ is a good place to find what other people are using."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "href": "posts/sd/stable-diffusion-1.html#refine-your-image-using-image2image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Refine your image using Image2Image",
    "text": "Refine your image using Image2Image\nThe image to image pipeline allows you to start from an initial image rather than a random one.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-2-base\" #@param [\"stabilityai/stable-diffusion-2-base\", \"stabilityai/stable-diffusion-2\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {type:\"string\"}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(\n    model_id, revision=\"fp16\", torch_dtype=torch.float16).to(device)\n\n\n\n\nMost of the Image2Image pipeline parameters should look similar, except:\n\ninit_image this is the image you start from, i.e. the image you want to fix.\nstrength and steps: strength is a number between 0-1, meaning how much your output depends on your init image. For example a 0.6 strength for 100 steps means doing diffusion starting from 40% of the init image, and then do 60 steps.\n\n\nimport PIL\nimage = PIL.Image.open('image_to_fix.jpg')\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 200\nstrength=0.6\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\nimages = pipe_i2i(\n    init_image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    strength=strength,\n    num_inference_steps=num_inference_steps,\n    generator=generator,\n    guidance_scale=guidance_scale,\n    num_images_per_prompt=num_images_per_prompt\n).images\n\ndisplay(image)\ndisplay(image_grid(images))\n\n\n\n\n\n\n\n\n\n\nNote that for the image2image, I used way more steps - since I want to improve the quality.\nNow comparing the generated images to the initial one, there are some images that are better. The second one is already quite good, while the first one is good except for the eyes. How can we fix that? Let’s first save it:\n\nimages[0].save('image_fix_eye.jpg')"
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "href": "posts/sd/stable-diffusion-1.html#fixing-your-image-using-inpainting",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Fixing your image using Inpainting",
    "text": "Fixing your image using Inpainting\nThe in-painting pipeline allows you to mask a region of the image, and re-generate only that region. Thus this is useful for adding small fixes to our images.\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, StableDiffusionInpaintPipeline, EulerAncestralDiscreteScheduler\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-inpainting\" #@param [\"stabilityai/stable-diffusion-2-inpainting\", \"runwayml/stable-diffusion-inpainting\"] {type:\"string\"}\n\nif model_id == \"runwayml/stable-diffusion-inpainting\":\n  inp_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n      model_id,\n      revision=\"fp16\",\n      torch_dtype=torch.float16,\n      use_auth_token=True\n  ).to(\"cuda\")\nelif model_id == \"stabilityai/stable-diffusion-2-inpainting\":\n  inp_pipe = DiffusionPipeline.from_pretrained(\n        model_id,\n        revision=\"fp16\",\n        torch_dtype=torch.float16,\n        # scheduler=scheduler # TODO currently setting scheduler here messes up the end result. A bug in Diffusers🧨\n      ).to(\"cuda\")\n  inp_pipe.scheduler = DPMSolverMultistepScheduler.from_config(inp_pipe.scheduler.config)\n  inp_pipe.enable_attention_slicing()\n  # inp_pipe.enable_xformers_memory_efficient_attention()\n\ninp_pipe.scheduler = EulerAncestralDiscreteScheduler(\n    num_train_timesteps=1000,\n    beta_end=0.012,\n    beta_start=0.00085,\n    beta_schedule=\"linear\",\n)\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\nTo use the in-painting pipeline, there is only one new paramter: - mask_image this is a black-white image where white pixels will be repainted, and black pixels will be preserved.\nFor the purpose of fixing our image, we just use the same prompts for generating this image. To ease the generation of a mask, we can use the gradio Web UI.\n\nimport gradio as gr\n\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nnum_images_per_prompt = 4\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\n\n\ndef inpaint(input_image):\n  image, mask = input_image['image'], input_image['mask']\n  image = image.resize((512,512))\n  mask = mask.resize((512,512))\n\n  print('image:')\n  display(image)\n  print('mask:')\n  display(mask)\n\n  images = inp_pipe(\n      prompt=prompt,\n      negative_prompt=negative_prompt,\n      image=image,\n      mask_image=mask.convert('RGB'),\n      width=512,\n      height=512,\n      generator=generator,\n      guidance_scale=guidance_scale,\n      num_inference_steps=num_inference_steps,\n      num_images_per_prompt=num_images_per_prompt,\n  ).images\n\n  result = image_grid(images)\n  print('result:')\n  display(result)\n  return result\n\nwith gr.Blocks() as demo:\n  gr.Markdown('In painting demo')\n\n  with gr.Row():\n    with gr.Column():\n      input_image = gr.Image(label='Input', type = 'pil', tool='sketch', height=1024)\n      button = gr.Button('Inpaint')\n    with gr.Column():\n      output_image = gr.Image(label='Output', height=1024)\n\n  button.click(inpaint, inputs=input_image, outputs=output_image)\n\ndemo.launch(debug=True, share=True)\n\nBy Default the in painting works terrible when fixing a small region:\n\n\n\n\n\n\n\n\nBut If we apply a small trick: zoom in on the area, then fix a larger portion, it will do a much better job:\n\n\n\n\n\n\n\n\nThus, if we can programmatically “zoom in”, fix the region, then paste to the original image, we can do a much better job. This is left to the reader as an interesting excercise."
  },
  {
    "objectID": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "href": "posts/sd/stable-diffusion-1.html#upsampling-your-image",
    "title": "Stable Diffusion from Begginer to Master (1) Pipelines and Prompts",
    "section": "Upsampling your image",
    "text": "Upsampling your image\nYou can boost our image resolution even higher by using the upsampler. Note this takes a lot of GPU memory.\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\nupscale_pipe = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-x4-upscaler\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16).to(\"cuda\")\n\nWARNING:root:WARNING: /usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\nNeed to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop\n\n\n/usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.8/dist-packages/diffusers/models/attention.py:433: UserWarning: Could not enable memory efficient attention. Make sure xformers is installed correctly and a GPU is available: No such operator xformers::efficient_attention_forward_generic - did you forget to build xformers with `python setup.py develop`?\n  warnings.warn(\n\n\n\nimport PIL\nprompt = \"a photo of a woman wearing a red dress, perfect face, dramatically lit,depth of field,smooth gradients\"\nnegative_prompt = \"disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal\"\nseed = 42\nnum_inference_steps = 30\nguidance_scale = 7.5\ngenerator = torch.Generator(device=\"cuda\").manual_seed(seed)\nimage = PIL.Image.open('low_res.png').convert('RGB').resize((128,128))\n\nupscale_image = upscale_pipe(\n    image=image,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    generator=generator\n).images[0]\n\ndisplay(image)\ndisplay(upscale_image)"
  },
  {
    "objectID": "posts/sd/stable-diffusion-3.html",
    "href": "posts/sd/stable-diffusion-3.html",
    "title": "Stable Diffusion from Begginer to Master (3) VAE Deep Dive",
    "section": "",
    "text": "Starting from this 3rd tutorial of Stable Diffusion, we’ll dive into the details of pipelines and see how each component work.\nIn this tutorial we’ll breifly have a look at what components are there in a Pipeline, then take a deeper dive into one of the component - the Variationanl Auto Encoder(VAE).\n\nSetup\n\n!pip install -Uqq diffusers transformers ftfy accelerate bitsandbytes\n#!pip install -Uqq triton xformers\n\n\n\nThe Building Blocks of Stable Diffusion Pipeline\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npipe = StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', revision=\"fp16\", torch_dtype=torch.float16).to(device)\n\n\n\n\nLet’s take a look at what is inside a StableDiffusionPipeline:\n\npipe\n\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.11.0.dev0\",\n  \"feature_extractor\": [\n    null,\n    null\n  ],\n  \"requires_safety_checker\": false,\n  \"safety_checker\": [\n    null,\n    null\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n\n\nThere are 5 main components of a StableDiffusionPipeline:\n\nVAE This is a downsampler + upsampler combination which works in the beginning and the end of the pipeline. It can “compress” the image from 512x512 -> 64x64 (8 times, which means the image sizes need to be divisible by 8) or “decompress” it back.\nText Encoder and Tokenizer This two components work together to convert text prompts into embeddings.\nUNet This is the main module for diffusion, which takes a nosiy image and a text embedding as the input, and predicts the noise.\nScheduler This determines the details of the diffusion process, e.g. how many steps to run, how much noise to remove at each step, etc.\n\n\n\nEncode and Decode with VAE\nThe VAE is a encoder-decoder combo, which can downsample an image to a smaller image(called latents) and then convert it back.\nWhy is this important? An image with 512x512 is worth 786432 numbers, combining that with convolutions(matrix multiplications) can use a lot of GPU memory and power. By downsampling it into a much smaller image, doing image generation on that, and then upsampling it to a large image can save a lot of memory and computing power. Although not necessary at all from a mathematical standpoint, the VAE is actually the key part that makes it possible to run stable diffusion on low-end GPUs, even personal computers.\n\nimport math\nimport PIL\nimport torch\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\ndef preprocess(image):\n  w, h = image.size\n  w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n  # PIL version compatiblity\n  try:\n    resample = PIL.Image.Resampling.LANCZOS\n  except:\n    resample = PIL.Image.LANCZOS\n  image = image.resize((w, h), resample=resample)\n  image = np.array(image).astype(np.float32) / 255.0\n  image = image[None].transpose(0, 3, 1, 2)\n  image = torch.from_numpy(image)\n  return 2.0 * image - 1.0\n\n\ndef numpy_to_pil(images):\n  \"\"\"\n  Convert a numpy image or a batch of images to a PIL image.\n  \"\"\"\n  if images.ndim == 3:\n      images = images[None, ...]\n  images = (images * 255).round().astype(\"uint8\")\n  if images.shape[-1] == 1:\n      # special case for grayscale (single channel) images\n      pil_images = [PIL.Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n  else:\n      pil_images = [PIL.Image.fromarray(image) for image in images]\n\n  return pil_images\n\n\n@torch.no_grad()\ndef encode_image(pipe, image, dtype=torch.float16):\n  if isinstance(image, PIL.Image.Image):\n    image = preprocess(image)\n  image = image.to(device=device, dtype=dtype)\n  latent_dist = pipe.vae.encode(image).latent_dist\n  latents = latent_dist.sample(generator=None)\n  latents = 0.18215 * latents\n  return latents\n\n\n@torch.no_grad()\ndef decode_latents(pipe, latents):\n  latents = 1 / 0.18215 * latents\n  image = pipe.vae.decode(latents).sample\n  image = (image / 2 + 0.5).clamp(0, 1)\n  # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n  image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n  image = numpy_to_pil(image)\n  if len(image) == 1:\n    # single image\n    return image[0]\n  return image\n\n\ndef image_grid(imgs, rows=None, cols=None) -> PIL.Image.Image:\n  n_images = len(imgs)\n  if not rows and not cols:\n    cols = math.ceil(math.sqrt(n_images))\n  if not rows:\n    rows = math.ceil(n_images / cols)\n  if not cols:\n    cols = math.ceil(n_images / rows)\n\n  w, h = imgs[0].size\n  grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n\n  for i, img in enumerate(imgs):\n      grid.paste(img, box=(i%cols*w, i//cols*h))\n  return grid\n\nNow let’s try out the VAE. 1. The original image is (516, 484) 2. preprocess will resize it to the nearest size divisible by 8, which is (512, 480). 3. encode_image converts image to latents with 4 channels, each with (60,64), which is 1/8 the original size. Note the x and y switches only because image size is (width, height) but array shape is (height, width). 4. decode_latents converts latents back to image, which is (512, 480).\n\ndef demo_vae(image, latents, decoded_image):\n  fig, axes = plt.subplots(figsize=(8,4), nrows=1, ncols=2)\n  plt.tight_layout()\n  axes[0].imshow(image)\n  # axes[0].set_axis_off()\n  axes[0].set_title(f'Original {image.size}')\n  axes[1].imshow(decoded_image)\n  # axes[1].set_axis_off()\n  axes[1].set_title(f'Decoded {decoded_image.size}')\n\n  if len(latents.shape) == 4:\n    latents = latents[0]\n  latents = latents.float().cpu().numpy()\n  n_channels = latents.shape[0]\n\n  fig, axs = plt.subplots(1, n_channels, figsize=(n_channels*4, 4))\n  fig.suptitle(f'Latent channels {latents.shape}') \n\n  for c in range(n_channels):\n    axs[c].imshow(latents[c], cmap='Greys')\n\nimage = PIL.Image.open('images/pikachu.png').convert('RGB')\nlatents = encode_image(pipe, image)\ndecoded_image = decode_latents(pipe, latents)\n\ndemo_vae(image, latents, decoded_image)\n\n\n\n\n\n\n\n\n\nFun experiment: adding some random noise to latents\nWhat if we add some random noise to the latents? Will VAE still be able to restore the original image?\n\ndef add_rand_noise(latents, noise_level):\n  scale = torch.mean(torch.abs(latents))\n  return latents + (torch.rand_like(latents)-0.5) * scale * noise_level\n\nimage = PIL.Image.open('images/pikachu.png').convert('RGB')\nlatents = encode_image(pipe, image)\n\nfig, axs = plt.subplots(1, 1, figsize=(4, 4))\naxs.imshow(image)\naxs.set_title('Original')\n# axs.set_axis_off()\n\ncount = 6\nnoise_levels = np.linspace(0.1,1,count)\nfig, axs = plt.subplots(1, count, figsize=(4*count, 4))\nfor ax, noise in zip(axs, noise_levels): \n  latents1 = add_rand_noise(latents, noise)\n  decoded_image = decode_latents(pipe, latents1)\n  ax.imshow(decoded_image)\n  ax.set_title(f'Noised {noise}')\n  # ax.set_axis_off()\n\n\n\n\n\n\n\nAs you can see in the above, VAE is quite resilient to noises. Even when adding roughly 50% noise with respect to the mean latent value scale(!!!), it can still do quite a good job of restoring the image.\n\n\nPlaying with VAE code\nNow we’ll dive into the VAE code to see what is going on. To do this in colab, we can add some print statements to diffusers/model/vae.py\nTo relect our changes without having to restart the colab runtime, we can run the following reload code:\n\nfrom diffusers.models import vae\nimport importlib\nimportlib.reload(vae)\npipe.vae.__class__ = vae.AutoencoderKL\n\nThe encode function encodes an image of shape [1,3,h,w] into a latent tensor [1,4,h/8,w/8].\ndef encode(self, x: torch.FloatTensor, return_dict: bool = True) -> AutoencoderKLOutput:\n    # x: [1, 3, h, w]\n\n    # h: [1, 8, h/8, w/8]\n    h = self.encoder(x)\n    \n    # moments: [1, 8, h/8, w/8]\n    moments = self.quant_conv(h)\n    \n    # posterior.std: [1, 4, h/8, w/8]\n    # posterior.var: [1, 4, h/8, w/8]\n    posterior = DiagonalGaussianDistribution(moments)\n\n    if not return_dict:\n        return (posterior,)\n\n    return AutoencoderKLOutput(latent_dist=posterior)\n\nThe encoder is a UNet that converts from input image x with shape [1,3,h,w] into a tensor h with shape [1,8,h/8,w/8].\nquant_conv is a Conv2d block with kernel size 1, which converts h into moments preserving the shape.\nThe moments with shape [1,8,h/8,w/8] is used as the standard deviation and variance of a distribution, from which the latents are sampled.\n\n\nfrom diffusers.models.vae import DiagonalGaussianDistribution\n\nimage = PIL.Image.open('images/pikachu.png').convert('RGB')\nprint('image:', image.size)\nx = preprocess(image).to(device=device, dtype=torch.float16)\nprint('image tensor:', x.shape)\nh = pipe.vae.encoder(x)\nprint('h:', h.shape)\nmoments = pipe.vae.quant_conv(h)\nprint('moments:', moments.shape)\nlatent_dist = DiagonalGaussianDistribution(moments)\nlatents = latent_dist.sample(generator=None)\nlatents = 0.18215 * latents\nprint('latents:', latents.shape)\n\nimage: (516, 484)\nimage tensor: torch.Size([1, 3, 480, 512])\nh: torch.Size([1, 8, 60, 64])\nmoments: torch.Size([1, 8, 60, 64])\nlatents: torch.Size([1, 4, 60, 64])\n\n\nNow the decode function:\ndef _decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\n    z = self.post_quant_conv(z)\n    dec = self.decoder(z)\n\n    if not return_dict:\n        return (dec,)\n\n    return DecoderOutput(sample=dec)\n\ndef decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:\n    if self.use_slicing and z.shape[0] > 1:\n        decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]\n        decoded = torch.cat(decoded_slices)\n    else:\n        decoded = self._decode(z).sample\n\n    if not return_dict:\n        return (decoded,)\n\n    return DecoderOutput(sample=decoded)\n\npost_quant_conv is a Conv2d with input channel and output channel equal to the latent channels (=4), kernel size 1. It converts latents to z with the same shape.\ndecoder is a UNet that converts from latent shape [1,4,h/8,w/8] into an image tensor of shape [1,3,h,w].\n\n\nlatents = 1 / 0.18215 * latents\nprint('latents:', latents.shape)\nz = pipe.vae.post_quant_conv(latents)\nprint('z:', z.shape)\ndec = pipe.vae.decoder(z)\nprint('dec:', dec.shape)\nimage = (dec.detach() / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).float().numpy()\nimage = numpy_to_pil(image)[0]\ndisplay(image)\n\nlatents: torch.Size([1, 4, 60, 64])\nz: torch.Size([1, 4, 60, 64])\ndec: torch.Size([1, 3, 480, 512])\n\n\n\n\n\nWe won’t dive deeper into the Encoder and Decoder (both are UNet, which is a special kind of Convolutional network, characterized by first scaling down and then scaling up feature maps). But you can defintely have a look at the source code to dig more details.\n\npipe.vae.encoder\n\nEncoder(\n  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (down_blocks): ModuleList(\n    (0): DownEncoderBlock2D(\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n        )\n      )\n    )\n    (1): DownEncoderBlock2D(\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n        )\n      )\n    )\n    (2): DownEncoderBlock2D(\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n        )\n      )\n    )\n    (3): DownEncoderBlock2D(\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2D(\n    (attentions): ModuleList(\n      (0): AttentionBlock(\n        (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n        (query): Linear(in_features=512, out_features=512, bias=True)\n        (key): Linear(in_features=512, out_features=512, bias=True)\n        (value): Linear(in_features=512, out_features=512, bias=True)\n        (proj_attn): Linear(in_features=512, out_features=512, bias=True)\n      )\n    )\n    (resnets): ModuleList(\n      (0): ResnetBlock2D(\n        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n      (1): ResnetBlock2D(\n        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n)"
  },
  {
    "objectID": "posts/python/python-parallel-processing.html",
    "href": "posts/python/python-parallel-processing.html",
    "title": "Python Parellel Processing",
    "section": "",
    "text": "I came across this function called parallel in fastai, and it seems very interesting.\n\n\n\n\n\n\nA Simple Example\n\nfrom fastcore.all import parallel\n\n\nfrom nbdev.showdoc import doc\n\n\ndoc(parallel)\n\nparallel[source]parallel(f, items, *args, n_workers=8, total=None, progress=None, pause=0, **kwargs)\n\nApplies func in parallel to items, using n_workers\nShow in docs\n\n\nAs the documentation states, the parallel function can run any python function f with items using multiple workers, and collect the results.\nLet’s try a simple examples:\n\nimport math\nimport time\n\ndef f(x):\n  time.sleep(1)\n  return x * 2\n\nnumbers = list(range(10))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 s\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 32 ms, sys: 52 ms, total: 84 ms\nWall time: 2.08 s\n\n\nThe function f we have in this example is very simple: it sleeps for one second and then returns x*2. When executed in serial, it takes 10 seconds which is exactly what we expect. When using more workers(8 by default), it takes only 2 seconds.\n\n\nDig into the Implementation\nLet’s see how parallel is implemented:\n\nparallel??\n\n\nSignature:\nparallel(\n    f,\n    items,\n    *args,\n    n_workers=8,\n    total=None,\n    progress=None,\n    pause=0,\n    **kwargs,\n)\nSource:   \ndef parallel(f, items, *args, n_workers=defaults.cpus, total=None, progress=None, pause=0, **kwargs):\n    \"Applies `func` in parallel to `items`, using `n_workers`\"\n    if progress is None: progress = progress_bar is not None\n    with ProcessPoolExecutor(n_workers, pause=pause) as ex:\n        r = ex.map(f,items, *args, **kwargs)\n        if progress:\n            if total is None: total = len(items)\n            r = progress_bar(r, total=total, leave=False)\n        return L(r)\nFile:      /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:      function\n\n\n\n\n\n??ProcessPoolExecutor\n\n\nInit signature:\nProcessPoolExecutor(\n    max_workers=8,\n    on_exc=<built-in function print>,\n    pause=0,\n    mp_context=None,\n    initializer=None,\n    initargs=(),\n)\nSource:        \nclass ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n    \"Same as Python's ProcessPoolExecutor, except can pass `max_workers==0` for serial execution\"\n    def __init__(self, max_workers=defaults.cpus, on_exc=print, pause=0, **kwargs):\n        if max_workers is None: max_workers=defaults.cpus\n        self.not_parallel = max_workers==0\n        store_attr(self, 'on_exc,pause,max_workers')\n        if self.not_parallel: max_workers=1\n        super().__init__(max_workers, **kwargs)\n    def map(self, f, items, *args, **kwargs):\n        self.lock = Manager().Lock()\n        g = partial(f, *args, **kwargs)\n        if self.not_parallel: return map(g, items)\n        try: return super().map(partial(_call, self.lock, self.pause, self.max_workers, g), items)\n        except Exception as e: self.on_exc(e)\nFile:           /opt/conda/lib/python3.7/site-packages/fastcore/utils.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we can see in the source code, under the hood, this is using the concurrent.futures.ProcessPoolExecutor class from Python.\nNote that this class is essentially different than Python Threads, which is subject to the Global Interpreter Lock.\nThe ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned.\n\n\nUse cases\nThis function can be quite useful for long running tasks and you want to take advantage of multi-core CPUs to speed up your processing. For example, if you want to download a lot of images from the internet, you may want to use this to parallize your download jobs.\nIf your function f is very fast, there can be suprising cases, here is an example:\n\nimport math\nimport time\n\ndef f(x):\n  return x * 2\n\nnumbers = list(range(10000))\n\n\n%%time\n\nlist(map(f, numbers))\nprint()\n\n\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 1.24 ms\n\n\n\n%%time\n\nlist(parallel(f, numbers))\nprint()\n\n\n\n\n\nCPU times: user 3.96 s, sys: 940 ms, total: 4.9 s\nWall time: 12.4 s\n\n\nIn the above example, f is very fast and the overhead of creating a lot of tasks outweigh the advantage of multi-processing. So use this with caution, and always take profiles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chi’s Blog",
    "section": "",
    "text": "Stable Diffusion from Begginer to Master (4) Text Encoder and Tokenizer\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion from Begginer to Master (3) VAE Deep Dive\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion from Begginer to Master (2) Dreambooth\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion from Begginer to Master (1) Pipelines and Prompts\n\n\n\n\n\n\n\nDeep Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPython Parellel Processing\n\n\n\n\n\n\n\nPython\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Chi is currently at Google, before which he has worked for Xiaomi, Amazon and PDD as a software engineer. His experiences include data and machine learning, especially in Ads and NLP."
  }
]